{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\pr}{\\textrm{Pr}}$\n",
    "$\\newcommand{\\l}{\\left}$\n",
    "$\\newcommand{\\r}{\\right}$\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\var}{\\textrm{Var}}$\n",
    "$\\newcommand{\\mc}{\\mathcal}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\lb}{\\left\\{}$\n",
    "$\\newcommand{\\rb}{\\right\\}}$\n",
    "$\\newcommand{\\iid}{\\textrm{i.i.d. }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Belief functions and probabilities\n",
    "\n",
    "Probabilities are a way to numerically express rational beliefs.\n",
    "This first section shows that probabilities satisfy some general\n",
    "features that we would expect a measure of \"belief\" to have, so\n",
    "it seems reasonable to use probabilities to represent our belief\n",
    "in something.\n",
    "\n",
    "Probability axioms:\n",
    "\n",
    "**P1**: $0 = \\pr\\l(\\textrm{not } H \\given H\\r) \\leq \\pr\\l(F \\given H\\r) \\leq \\pr\\l(H \\given H\\r) = 1$\n",
    "\n",
    "**P2**: $\\pr\\l(F \\cup G \\given H\\r) = \\pr\\l(F \\given H\\r) + \\pr\\l(G \\given H\\r)$ if $F \\cap G = \\varnothing$\n",
    "\n",
    "**P3**: $\\pr\\l(F \\cap G \\given H\\r) = \\pr\\l(G \\given H\\r)\\pr\\l(F \\given G \\cap H\\r)$\n",
    "\n",
    "\n",
    "# 2.2 Events, partitions, and Bayes' rule\n",
    "\n",
    "**Definition 1 (Partition)**: A collection of sets $\\left\\{H_1, \\cdots, H_k\\right\\}$ is a partition\n",
    "of another set $\\mathcal{H}$ if \n",
    "\n",
    "1. the events are disjoint: $H_i \\cap H_j = \\varnothing$ for $i \\not= j$\n",
    "2. the union of the sets is $\\mathcal{H}$: $\\bigcup_{k=1}^K H_k = \\mathcal{H}$\n",
    "\n",
    "If $\\mathcal{H}$ is the set of all possible truths and $\\left\\{H_1, \\cdots, H_k\\right\\}$ is a partition of \n",
    "$\\mathcal{H}$, then exactly one of $\\left\\{H_1, \\cdots, H_k\\right\\}$ contains the truth.\n",
    "\n",
    "Suppose $\\left\\{H_1, \\cdots, H_k\\right\\}$ is a partition of $\\mathcal{H}$, $\\textrm{Pr}\\left(\\mathcal{H}\\right) = 1$,\n",
    "and $E$ is some specific event. The axioms of probability imply:\n",
    "\n",
    "**Rule of total probability**: $$\\sum_{k=1}^{K} \\pr\\left(H_k\\right) = 1$$\n",
    "\n",
    "**Rule of marginal probability**: \n",
    "\\begin{align}\n",
    "\\pr\\left(E\\right) &= \\sum_{k=1}^K \\pr\\left(E \\cap H_k\\right) \\\\\n",
    "&= \\sum_{k=1}^K \\pr\\left(E \\given H_k\\right)\\pr\\left(H_k\\right)\n",
    "\\end{align}\n",
    "\n",
    "**Bayes' rule**:\n",
    "\\begin{align}\n",
    "\\pr\\left(H_j \\given E\\right) &= \\frac{\\pr\\l(E \\given H_j\\r)\\pr\\l(H_j\\r)}{\\pr\\l(E\\r)} \\\\\n",
    "&= \\frac{\\pr\\l(E \\given H_k\\r)\\pr\\l(H_j\\r)}{\\sum_{k=1}^K \\pr\\l(E \\given H_k\\r)\\pr\\l(H_k\\r)}\n",
    "\\end{align}\n",
    "\n",
    "We would say that $H_k$ has been marginalized out in the denominator.\n",
    "\n",
    "I think it's worth looking at the formula for Bayes' rule here. The left hand side, \n",
    "$\\pr\\left(H_j \\given E\\right)$, represents that probability that $H_j$ is true given that\n",
    "event $E$ happened. For us, $E$ is generally data and $H_k$ will be a parameter we are \n",
    "interested in estimating. On the right side, we have $\\pr\\l(E \\given H_k\\r)$\n",
    "which is the probability of observing the event/data under the parameterization $H_k$.\n",
    "This is multiplied by $\\pr\\l(H_j\\r)$ which is our prior distribution on $H_j$.\n",
    "The denominator is a little more tricky. $\\pr\\l(E\\r)$ doesn't mean much on its own. However,\n",
    "we can rewrite the denominator using the rule of marginal probability as\n",
    "\n",
    "$$\\pr\\l(E\\r) = \\sum_{k=1}^K\\pr\\l(E \\cap H_k\\r) = \\sum_{k=1}^K \\pr\\l(E \\given H_k\\r)\\pr\\l(H_k\\r)$$\n",
    "\n",
    "Since we think $E$ depends on the $H_i$, we can treat $\\pr\\l(E\\r)$ as a \n",
    "**marginal density** and rewrite it as a joint distribution (see 2.5 below for more information).\n",
    "We know $\\pr\\l(E \\given H_k\\r)$ and $\\pr\\l(H_k\\r)$ so we can evaluate this expression.\n",
    "\n",
    "I kind of think of\n",
    "the numerator as the strength of my belief and the denominator acts to normalize that value\n",
    "based on how strongly I believe all of the $H_k$.\n",
    "\n",
    "**Bayes factors**: $\\l\\{H_1, \\cdots, H_k\\r\\}$ often refer to disjoint hypotheses and $E$ refers\n",
    "to data. To compare hypotheses post-experimentally, we often calculate the ratio:\n",
    "\\begin{align}\n",
    "\\frac{\\pr\\l(H_i \\given E\\r)}{\\pr\\l(H_j \\given E\\r)} &= \n",
    "\\frac{\\pr\\l(E \\given H_i\\r)}{\\pr\\l(E \\given H_j\\r)} \\times \\frac{\\pr\\l(H_i\\r)}{\\pr\\l(H_j\\r)} \\\\\n",
    "&= \\textrm{\"Bayes factor\"}\\times \\textrm{\"prior beliefs\"}\n",
    "\\end{align}\n",
    "\n",
    "This quantity reminds us that Bayes' rule does not determine what our beliefs should be after we \n",
    "see data, it only tells us how they should change. \n",
    "\n",
    "# 2.3 Independence\n",
    "\n",
    "**Definition 2 (Independence)**: Two events $F$ and $G$ are conditionally independent given\n",
    "$H$ if $\\pr\\l(F \\cap G \\given H\\r) = \\pr\\l(F \\given H\\r)\\pr\\l(G \\given H\\r)$.\n",
    "\n",
    "If $F$ and $G$ are conditionally independent, then knowing $G$ will tell us nothing about $F$.\n",
    "\n",
    "# 2.4 Random variables\n",
    "\n",
    "A random variable is an unknown numerical quantity about which we make probability statements.\n",
    "\n",
    "## 2.4.1 Discrete random variables\n",
    "\n",
    "Let $Y$ be a random variable and let $\\mathcal{Y}$ be the set of all possible values of $Y$.\n",
    "We say $Y$ is discrete if $\\mathcal{Y}$ is countable: $\\mathcal{Y} = \\l\\{y_1, y_2, \\cdots\\r\\}$.\n",
    "\n",
    "For short, we will write $\\pr\\l(Y=y\\r) = p\\l(y\\r)$ where $p$ is the probability density function (pdf).\n",
    "The pdf has the following properties:\n",
    "\n",
    "1. $0 \\leq p\\l(y\\r) \\leq 1$ for all $y \\in \\mathcal{Y}$\n",
    "2. $\\sum_{y \\in \\mathcal{Y}} p\\left(y\\right) = 1$\n",
    "\n",
    "## 2.4.2 Continuous random variables\n",
    "\n",
    "If the sample space $\\mathcal{Y}$ is roughly equal to $\\mathbb{R}$, then we often define\n",
    "probability distributions for random variables in terms of a cumulative distribution function\n",
    "(cdf): $F\\l(y\\r) = \\pr\\l(Y \\leq y\\r)$. Note that\n",
    "\n",
    "* $F\\l(\\infty\\r) = 1$\n",
    "* $F\\l(-\\infty\\r) = 0$\n",
    "* $F\\l(b\\r) \\leq F\\l(a\\r)$ if $b<a$\n",
    "* $\\pr\\l(Y > a\\r) = 1-F\\l(a\\r)$\n",
    "* $\\pr\\l(a < Y < b\\r) = F\\l(b\\r) - F\\l(a\\r)$\n",
    "\n",
    "If $Y$ is a continuous random variable, then there exists a pdf $p$ such that\n",
    "$$F\\l(a\\r) = \\int_{-\\infty}^a p\\l(y\\r)dy$$\n",
    "\n",
    "The continuous pdf has analogous characteristics to the discrete pdf:\n",
    "\n",
    "1. $0 \\leq p\\l(y\\r) \\leq 1$ for all $y \\in \\mathcal{Y}$\n",
    "2. $\\int_{y \\in \\mathbb{R}} p\\left(y\\right)dy = 1$\n",
    "\n",
    "## 2.4.3 Descriptions of distributions\n",
    "\n",
    "\n",
    "### Mean, mode, median\n",
    "\n",
    "The **mean** or **expectation** of an unknown quantity $Y$ is given by\n",
    "\n",
    "* $E\\l[Y\\r] = \\sum_{y\\in Y}yp\\l(y\\r)$ if $Y$ is discrete\n",
    "* $E\\l[Y\\r] = \\int_{y\\in \\mathbb{R}}yp\\l(y\\r)dy$ if $Y$ is continuous\n",
    "\n",
    "The mean is the center of mass of the distribution. It is generally not equal to\n",
    "\n",
    "* the **mode**: the probable value of $Y$\n",
    "* the **median**: the value of $Y$ in the middle of the distribution\n",
    "\n",
    "The mean is a good quantity to look at because\n",
    "\n",
    "1. The mean is a scaled value of the total of $\\l\\{Y_1, \\cdots, Y_n\\r\\}$, and the total\n",
    "is often a quantity of interest.\n",
    "2. If you were forced to guess the value of $Y$, guessing the mean would minimize your error\n",
    "if it was measured as $\\l(Y - y_{guess}\\r)^2$.\n",
    "3. In some simple models, the mean contains all of the information about the population that \n",
    "can be obtained from the data.\n",
    "\n",
    "### Variance\n",
    "\n",
    "The variance is a measure of the spread:\n",
    "\n",
    "\\begin{align}\n",
    "\\var\\l[Y\\r] &= E\\l[\\l(Y - E\\l[Y\\r]\\r)^2\\r] \\\\\n",
    "&= E\\l[Y^2\\r] - E\\l[Y\\r]^2\n",
    "\\end{align}\n",
    "\n",
    "The variance is the average squared distance that a sample value $Y$ will be from \n",
    "the population mean $E\\l[Y\\r]$. The standard deviation is the square root of the variance\n",
    "and is on the same scale as the mean.\n",
    "\n",
    "For a continuous, increasing cdf $F$, the $\\alpha$ **quantile** is the $y_\\alpha$ such that\n",
    "$F\\left(y_\\alpha\\right) \\equiv \\pr\\l(Y \\leq y_\\alpha\\r) = \\alpha$. The **interquartile** range\n",
    "is the interval $\\l(y_{0.25}, y_{0.75}\\r)$ which contains 50% of the mass of the distribution.\n",
    "\n",
    "## 2.5 Joint distributions\n",
    "\n",
    "### Discrete distributions\n",
    "\n",
    "Let \n",
    "\n",
    "* $\\mathcal{Y}_1$, $\\mathcal{Y}_2$ be two countable samples spaces\n",
    "* $Y_1$, $Y_2$, be two random variables, taking values in $\\mathcal{Y}_1$, $\\mathcal{Y}_2$ respectively\n",
    "\n",
    "The **joint pdf** or **joint density** of $Y_1$ and $Y_2$ is defined as \n",
    "$$p_{Y_1 Y_2}\\l(y_1, y_2\\r) = \\pr\\l(\\l\\{Y_1 = y_1\\r\\} \\cap \\l\\{Y_2 = y_2\\r\\} \\r)$$\n",
    "for $y_1 \\in \\mc{Y}_1$, $y_2 \\in \\mc{Y}_2$.\n",
    "\n",
    "The **marginal density** can be computed from the joint density:\n",
    "\\begin{align}\n",
    "p_{Y_1}\\l(y_1\\r) &\\equiv \\pr\\l(Y_1 = y_1\\r) \\\\\n",
    "&= \\sum_{y_2 \\in \\mc{Y}_2} \\pr\\l(\\l\\{Y_1 = y_1\\r\\} \\cap \\l\\{Y_2 = y_2\\r\\} \\r) \\\\\n",
    "&\\equiv \\sum_{y_2 \\in \\mc{Y}_2}p_{Y_1Y_2} \\l(y_1, y_2\\r)\n",
    "\\end{align}\n",
    "\n",
    "The **conditional density** of $Y_2$ given $\\l\\{Y_1 = y_1\\r\\}$ can be computed from the \n",
    "joint density and the marginal density of $Y_1$:\n",
    "\\begin{align}\n",
    "p_{Y_2 \\given Y_1}\\l(y_2 \\given y_1\\r) &= \\frac{\\pr\\l(\\l\\{Y_1 = y_1\\r\\} \\cap \\l\\{Y_2 = y_2\\r\\}\\r)}{\\pr\\l(Y_1 = y_1\\r)} \\\\\n",
    "&= \\frac{p_{Y_1Y_2}\\l(y_1, y_2\\r)}{p_{Y_1}\\l(y_1\\r)} \\\\\n",
    "&= \\frac{p_{Y_1Y_2}\\l(y_1, y_2\\r)}{\\sum_{y_2 \\in \\mc{Y}_2}p_{Y_1Y_2} \\l(y_1, y_2\\r)}\n",
    "\\end{align}\n",
    "\n",
    "We often drop the subscripts on the pdf's such that $p_{Y_1}\\l(y_1\\r)$ becomes $p\\l(y_1\\r)$ etc.\n",
    "\n",
    "### Continuous joint distributions\n",
    "\n",
    "If $Y_1$ and $Y_2$ are continuous, we have a joint cdf $F_{Y_1Y_2}\\l(a, b\\r) \\equiv \n",
    "\\pr\\l(\\l\\{Y_1 \\leq a\\r\\} \\cap \\l\\{Y_2 \\leq b\\r\\}\\r)$, there is a function $p_{Y_1Y_2}$ such that\n",
    "$$F_{Y_1Y_2}\\l(a,b\\r) = \\int_{-\\infty}^a \\int_{-\\infty}^b p_{Y_1Y_2}\\l(y_1, y_2\\r)dy_2dy_1$$\n",
    "\n",
    "The function $p_{Y_1Y_2}$ is the joint density of $Y_1$ and $Y_2$. As in the discrete case, \n",
    "we have\n",
    "* $p_{Y_1}\\l(y_1\\r) = \\int_{-\\infty}^\\infty p_{Y_1Y_2}\\l(y_1,y_2\\r)dy_2$\n",
    "* $p_{Y_2 \\given Y_1}\\l(y_2 \\given y_1\\r) = p_{Y_1Y_2}\\l(y_1, y_2\\r) / p_{Y_1}\\l(y_1\\r)$\n",
    "\n",
    "### Mixed continuous and discrete variables\n",
    "\n",
    "Let $Y_1$ be discrete and $Y_2$ be continuous. Suppose we have\n",
    "\n",
    "* a marginal density $p_{Y_1}$ from our beliefs $\\pr\\l(Y_1=y_1\\r)$\n",
    "* a conditional density $p_{Y_2 \\given Y_1}\\left(y_2\\given y_1\\r)$ from\n",
    "$\\pr\\l(Y_2 \\leq y_2 \\given Y_1 = y_1\\r) \\equiv F_{Y_2 \\given Y_1}\\l(y_2 \\given y_1\\r)$\n",
    "\n",
    "The joint density of $Y_1$ and $Y_2$ is then \n",
    "$$p_{Y_1Y_2}\\l(y_1, y_2\\r) = p_{Y_1}\\l(y_1\\r) \\times p_{Y_2 \\given Y_1}\\l(y_2 \\given y_1\\r)$$\n",
    "\n",
    "and has the property that\n",
    "$$\\pr\\l(Y_1 \\in A, Y_2 \\in B\\r) = \\int_{y_2 \\in B} \\left\\{\\sum_{y_1 \\in A} p_{Y_1Y_2}\\l(y_1, y_2\\r)\\r\\}dy_2$$\n",
    "\n",
    "In other words, we can use summation and integration to calculate the joint density.\n",
    "\n",
    "### Bayes rule and parameter estimation\n",
    "\n",
    "Let $\\theta$ be a continuous parameter we want to estimate and let $Y$ be a discrete \n",
    "data measurement. Bayesian estimation of $\\theta$ derives from the calculation of \n",
    "$p\\l(\\theta \\given y\\r)$, where $y$ is the observed value of $Y$. This calculation \n",
    "first requires the joint density of $\\theta$ and $Y$. We can construct the joint density from\n",
    "\n",
    "* $p\\l(\\theta\\r)$, beliefs about $\\theta$\n",
    "* $p\\l(y \\given \\theta\\r)$, beliefs about $Y$ for each value of $\\theta$\n",
    "\n",
    "Having observed $\\l\\{Y = y\\r\\}$, we need to compute our updated beliefs about $\\theta$:\n",
    "$$p\\l(\\theta | y\\r) = p\\l(\\theta, y\\r) / p\\lp y\\rp = p\\lp \\theta \\rp p\\lp y \\given \\theta \\rp / p\\lp y \\rp$$\n",
    "\n",
    "**This conditional density is called the posterior density of $\\theta$.** If $\\theta_a$ and $\\theta_b$\n",
    "are two estimates of $\\theta$, the posterior probability (density) of $\\theta_a$ relative to $\\theta_b$,\n",
    "conditional on $Y=y$, is \n",
    "\\begin{align}\n",
    "\\frac{p\\lp \\theta_a \\given y \\rp}{p \\lp \\theta_b \\given y \\rp} &= \n",
    "\\frac{p\\lp \\theta_a \\rp p\\lp y | \\theta_a \\rp / p\\lp y \\rp}{p\\lp \\theta_b \\rp p\\lp y | \\theta_b \\rp / p\\lp y \\rp} \\\\\n",
    "&= \\frac{p\\lp \\theta_a \\rp p\\lp y | \\theta_a \\rp}{p\\lp \\theta_b \\rp p\\lp y | \\theta_b \\rp}\n",
    "\\end{align}\n",
    "\n",
    "To evaluate the **relative** posterior odds of $\\theta_a$ and $\\theta_b$, we do not need to evaluate\n",
    "the marginal density $p\\lp y \\rp$. We see that\n",
    "$$p\\lp \\theta | y \\rp \\propto p\\lp \\theta \\rp p\\lp y \\given \\theta \\rp$$\n",
    "\n",
    "The constant of proportionality is $1 / p\\lp y \\rp$ which we *could* calculate using\n",
    "$$p\\lp y \\rp = \\int_\\Theta p\\lp y, \\theta \\rp d\\theta = \\int_\\Theta p\\lp y \\given \\theta \\rp p\\lp \\theta \\rp d\\theta$$\n",
    "\n",
    "Later we will see that the numerator is more important than this denominator.\n",
    "\n",
    "# 2.6 Independent random variables\n",
    "\n",
    "Suppose $Y_1, \\cdots, Y_n$ are random variables and that $\\theta$ is a parameter describing\n",
    "the conditions under which the random variables are generated. We say that $Y_1, \\cdots, Y_n$\n",
    "are **conditionally independent** given $\\theta$ if for every collection of $n$ sets $\\lb A_1, \\cdots, A_n \\rb$ \n",
    "we have \n",
    "$$\\pr\\lp Y_1 \\in A_1, \\cdots, Y_n \\in A_n \\given \\theta \\rp = \n",
    "\\pr\\lp Y_1 \\in A_1 \\given \\theta\\rp \\times \\cdots \\times \\pr \\lp Y_n \\in A_n \\given \\theta \\rp$$\n",
    "\n",
    "This tells us that knowing $Y_j$ gives us no further information about $Y_i$ beyond what $\\theta$\n",
    "gives us. Under independence, the joint density of the $Y_i$ condtioned on $\\theta$ is \n",
    "$$p\\lp y_1, y_2, \\cdots, y_n \\given \\theta \\rp = \\prod_{i=1}^n p_{Y_i} \\lp y_i \\given \\theta \\rp$$\n",
    "\n",
    "In this case, we say that $Y_1, \\cdots, Y_n$ are **conditionally independent and identically\n",
    "distributed (i.i.d.)**: $Y_1, \\cdots, Y_n \\sim \\iid p\\lp y \\given \\theta \\rp$.\n",
    "\n",
    "# 2.7 Exchangeability\n",
    "\n",
    "**Definition 3 (Exchangeable)**: Let $p\\lp y_1, \\cdots, y_n\\rp$ be the joint density of $Y_1, \\cdots, Y_n$.\n",
    "If $p\\lp y_1, \\cdots, y_n\\rp = p\\lp y_{\\pi_1}, \\cdots, y_{\\pi_n}\\rp$ for all permutations $\\pi$ of \n",
    "$\\lb 1, \\cdots, n \\rb$, then $Y_1, \\cdots, Y_N$ are exchangeable. This basically means that \n",
    "$Y_1, \\cdots, Y_N$ are exchangeable if the subscript labels convey no information about the outcomes.\n",
    "\n",
    "Therom: If $\\theta \\sim p\\lp \\theta \\rp$ and $Y_1, \\cdots, Y_N$ are conditionally i.i.d. given $\\theta$,\n",
    "then marginally (unconditionally on $\\theta$), $Y_1, \\cdots, Y_N$ are exchangeable.\n",
    "\n",
    "# 2.8 de Finetti's thereom\n",
    "\n",
    "We know that if $Y_1, \\cdots, Y_n \\given \\theta$ are i.i.d. and $\\theta \\sim p\\lp \\theta \\rp$, then\n",
    "$Y_1, \\cdots, Y_N$ are exchangeable. Can we say something about the other direction?\n",
    "\n",
    "**Theorem 1 (de Finetti)**: Let $\\lb Y_1, \\cdots, Y_n \\rb$ be a potentially\n",
    "infinite sequence of random variables all having a common sample space $\\mc{Y}$. \n",
    "Let $Y_i \\in \\mc{Y}$ for all $i \\in \\lb Y_1, Y_2, \\cdots \\rb$. Suppose that for \n",
    "any $n$ our belief model for $Y_1, \\cdots, Y_n$ is exchangeable\n",
    "$$p \\lp y_1, \\cdots, y_n \\rp = p\\lp y_{\\pi_1}, \\cdots, y_{\\pi_n} \\rp$$\n",
    "for all permutations of $\\lb 1, \\cdots, n \\rb$. The our model can be written as \n",
    "$$p\\lp y_1, \\cdots, y_n \\rp = \\int \\lb \\prod_1^np \\lp y_i \\given \\theta \\rp \\rb p \\lp \\theta \\rp d \\theta$$\n",
    "for some parameter $\\theta$, some prior distribution on $\\theta$ ($p \\lp \\theta \\rp$), and some sampling\n",
    "model $p\\lp y \\given \\theta \\rp$. The prior and sampling model depend on the form of the belief\n",
    "model $p\\lp y_1, \\cdots, y_n \\rp$.\n",
    "\n",
    "The probability distribution $p\\lp \\theta \\rp$ represents our beliefs about the outcomes\n",
    "of $\\lb Y_1, \\cdots, Y_n \\rb$, induced by our belief model $p\\lp y_1, y_2, \\cdots \\rp$.\n",
    "More precisely,\n",
    "\n",
    "* $p\\lp \\theta \\rp$ represents our beliefs about $\\lim_{n\\rightarrow \\infty} Y_i / n$ in the binary case\n",
    "* $p\\lp \\theta \\rp$ represents our beliefs about $\\lim_{n\\rightarrow \\infty} \\lp Y_i \\leq c \\rp / n$ \n",
    "for each $c$ in the general case\n",
    "\n",
    "We can summarize as\n",
    "$$Y_1, \\cdots, Y_n \\given \\theta \\textrm{ are i.i.d. and }\\theta \\sim p\\lp \\theta \\rp \\iff\n",
    "Y_1, \\cdots, Y_N \\textrm{ are exchangeable for all } n$$\n",
    "\n",
    "The question is when are $Y_1, \\cdots, Y_n$ exchangeable for all $n$? For this to be true,\n",
    "we need both exchangeability and repeatability. Exchangeability is true when the labels\n",
    "have no meaning. Repeatability is true when\n",
    "\n",
    "* $Y_1, \\cdots, Y_n$ are outcomes of a repeatable experiment\n",
    "* $Y_1, \\cdots, Y_n$ are sampled from a finite population *with* replacement\n",
    "* $Y_1, \\cdots, Y_n$ are sampled from an infinite population *without* replacement\n",
    "\n",
    "If $Y_1, \\cdots, Y_n$ are exchangeable and sampled from a finite population *without* replacement\n",
    "of size $N >> n$, then they can be modeled as approximately conditionally i.i.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
