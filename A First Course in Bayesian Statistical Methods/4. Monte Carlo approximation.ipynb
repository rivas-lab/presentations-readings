{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\pr}{\\textrm{Pr}}$\n",
    "$\\newcommand{\\l}{\\left}$\n",
    "$\\newcommand{\\r}{\\right}$\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\var}{\\textrm{Var}}$\n",
    "$\\newcommand{\\mc}{\\mathcal}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\lb}{\\left\\{}$\n",
    "$\\newcommand{\\rb}{\\right\\}}$\n",
    "$\\newcommand{\\iid}{\\textrm{i.i.d. }}$\n",
    "$\\newcommand{\\ev}{\\textrm{E}}$\n",
    "$\\newcommand{\\odds}{\\textrm{odds}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've looked at simple posterior distributions that we can evaluate\n",
    "exactly. However, sometimes we may be interested in more complex posteriors\n",
    "that we cannot evaluate numerically. We may also be interested in other quantities\n",
    "like the $\\pr\\lp \\theta \\in A \\given y_1, \\cdots, y_n\\rp$ for arbitrary sets $A$.\n",
    "We may also want to calculate the posterior of some function of $\\theta$ \n",
    "like $\\l|\\theta_1 - \\theta_2\\r|$. We can use Monte Carlo sampling to do this.\n",
    "\n",
    "# 4.1 The Monte Carlo method\n",
    "\n",
    "**Monte Carlo approximation** is based on random sampling. Let $\\theta$ be a parameter\n",
    "of interest and let $y_1, \\cdots, y_n$ be the numerical values of a sample from a distribution\n",
    "$p\\lp y_1, \\cdots, y_n \\given \\theta \\rp$. Suppose we take $S$ independent, random samples\n",
    "from the posterior distribution $p\\lp \\theta \\given y_1, \\cdots, y_n \\rp$:\n",
    "$$\\theta^{\\lp 1\\rp}, \\theta^{\\lp 2\\rp}, \\cdots, \\theta^{\\lp S\\rp} \\sim \\iid p\\lp \\theta \\given y_1, \\cdots, y_n \\rp$$\n",
    "Since we are sampling from the posterior, these $S$ samples will approximate the posterior,\n",
    "and the accuracy will increase as $S$ increases. The empirical distribution \n",
    "$\\lb\\theta^{\\lp 1\\rp}, \\theta^{\\lp 1\\rp}, \\cdots, \\theta^{\\lp S\\rp}\\rb$ is known as the \n",
    "**Monte Carlo approximation** to $p\\lp \\theta \\given y_1, \\cdots, y_n \\rp$. As $S\\rightarrow\\infty$,\n",
    "\n",
    "* $\\bar{\\theta} = \\sum_{s=1}^S \\theta^{\\lp s\\rp} / S \\rightarrow \\ev\\l[\\theta \\given y_1, \\cdots, y_n\\r]$ (the empirical mean of the samples goes to the expected value of the posterior)\n",
    "* $\\sum_{s=1}^S \\lp\\theta^{\\lp s\\rp} - \\bar{\\theta}\\rp^2 / \\lp S-1\\rp \\rightarrow \\var\\l[\\theta \\given y_1, \\cdots, y_n\\r]$ (the empirical variance of the sample goes to the variance of the posterior)\n",
    "* #$\\lp \\theta^{\\lp S\\rp} \\leq c\\rp / S \\rightarrow \\pr\\lp\\theta \\leq c \\given y_1, \\cdots, y_n\\rp$ (the empirical\n",
    "probability of observing a sample less than $c$ approximates the cdf of the posterior)\n",
    "* the empirical distribution of $\\lb \\theta^{\\lp 1\\rp}, \\cdots, \\theta^{\\lp S\\rp} \\rb\n",
    "\\rightarrow p\\lp \\theta \\given y_1, \\cdots, y_n \\rp$\n",
    "* the median of $\\lb \\theta^{\\lp 1\\rp}, \\cdots, \\theta^{\\lp S\\rp} \\rb \\rightarrow \\theta_{1/2}$\n",
    "* the $\\alpha$-percentile of $\\lb \\theta^{\\lp 1\\rp}, \\cdots, \\theta^{\\lp S\\rp} \\rb \\rightarrow \\theta_\\alpha$\n",
    "\n",
    "Almost any aspect of a posterior distribution can be approximated arbitrarily exactly using Monte Carlo,\n",
    "and this is true for almost all functions $g\\lp\\theta\\rp$ as well.\n",
    "\n",
    "You can look at plots of estimated values to see when you have performed enough Monte Carlo \n",
    "samples. For instance, you could plot the empirical mean of the samples (which would be the\n",
    "expected value of the posterior distribution). With enough samples, this estimate should\n",
    "converge to some value.\n",
    "\n",
    "If you are using the mean of the Monte Carlo samples $\\bar{\\theta}$ to estimate the mean of the posterior,\n",
    "the Central Limit Theorem says that $\\hat{\\theta}$ is approximately normally distributed with mean\n",
    "$\\ev\\l[\\theta\\given y_1,\\cdots,y_n\\r]$ and standard deviation $\\sqrt{\\var\\l[\\theta\\given y_1,\\cdots,y_n\\r] / S}$.\n",
    "The Monte Carlo standard error is the approximation to this standard deviation $\\sqrt{\\hat{\\sigma}^2 / S}$ where\n",
    "$\\hat{\\sigma}$ is the empirical standard deviation. An approximate 95% confidence interval for the posterior\n",
    "mean of $\\theta$ is $\\hat{\\theta} \\pm 2\\sqrt{\\hat{\\sigma}^2/S}$. You can choose $S$ large enough to get the\n",
    "desired precision for $\\theta$.\n",
    "\n",
    "# 4.2 Posterior inference for arbitrary functions\n",
    "\n",
    "Suppose we are interested in the posterior distribution of some computable function $g\\lp\\theta\\rp$ \n",
    "of $\\theta$. For the binomial model, we may be interested in the log odds\n",
    "$$\\log \\odds\\lp\\theta\\rp = \\log\\frac{\\theta}{1-\\theta} = \\gamma$$\n",
    "The law of large numbers says that if we sample from the posterior distribution of $\\theta$,\n",
    "then the average value of $\\log\\frac{\\theta^{\\lp S\\rp}}{1-\\theta^{\\lp S\\rp}}$ converges to\n",
    "$\\ev\\l[\\log\\frac{\\theta}{1-\\theta}\\given y_1, \\cdots, y_n\\r]$. You can also calculate\n",
    "$g\\lp\\theta^{\\lp 1\\rp}\\rp, \\cdots, g\\lp\\theta^{\\lp S\\rp}\\rp$ independently as you sample.\n",
    "From these samples, as $S\\rightarrow\\infty$, you get all of the information from the bullet\n",
    "points above (mean, variance, empirical distribution, median, $\\alpha$-percentile).\n",
    "\n",
    "# 4.3 Sampling from predictive distributions\n",
    "\n",
    "The predictive distribution of a random variable $\\tilde{Y}$ is a probability distribution for\n",
    "$\\tilde{Y}$ such that \n",
    "\n",
    "* known quantities have been conditioned out\n",
    "* unknown quantities have been integrated out\n",
    "\n",
    "A predictive distribution that integrates over unknown parameters but is not conditional\n",
    "on observed data is called a **prior predictive distribution**. Such a distribution\n",
    "can be useful for evaluating whether a prior distribution for $\\theta$ actually\n",
    "translates into reasonable prior beliefs for observable data $\\tilde{Y}$.\n",
    "\n",
    "After we observe data, the **posterior predictive distribution** is\n",
    "$$\\pr\\lp\\tilde{Y} = \\tilde{y} \\given Y_1=y_1, \\cdots, Y_n=y_n\\rp = \n",
    "\\int p\\lp\\tilde{y}\\given\\theta\\rp p\\lp\\theta\\given y_1, \\cdots, y_n\\rp d\\theta$$\n",
    "In the case of a Poisson model with a gamma prior, the posterior predictive distribution\n",
    "was a negative binomial.\n",
    "\n",
    "Often, we can sample from $p\\lp\\theta\\given y_1, \\cdots, y_n\\rp$ and $p\\lp y\\given\\theta\\rp$\n",
    "but it is too complicated to sample from the posterior predictive distribution. We can use\n",
    "a Monte Carlo procedure to sample indirectly from the posterior predictive distribution instead.\n",
    "So we want a set of samples from $\\tilde{Y}$ from its posterior  predictive distribution. We can\n",
    "do this by sampling \n",
    "\\begin{align}\n",
    "&\\textrm{sample } \\theta^\\lp 1\\rp \\sim p\\lp\\theta\\given y_1, \\cdots, y_n\\rp,\n",
    "\\textrm{sample } \\tilde{y}^\\lp 1\\rp \\sim p\\lp\\tilde{y}\\given \\theta^\\lp 1\\rp \\rp \\\\\n",
    "&\\vdots \\\\\n",
    "&\\textrm{sample } \\theta^\\lp S\\rp \\sim p\\lp\\theta\\given y_1, \\cdots, y_n\\rp,\n",
    "\\textrm{sample } \\tilde{y}^\\lp S\\rp \\sim p\\lp\\tilde{y}\\given \\theta^\\lp S\\rp \\rp\n",
    "\\end{align}\n",
    "In other words, you sample from the posterior of $\\theta$ and use that sample to sample\n",
    "$\\tilde{Y}$. These joint samples constitute $S$ indepdendent samples from the joint posterior\n",
    "distribution of $\\lp \\theta, \\tilde{Y}\\rp$, and the samples of $\\tilde{Y}$ constitute\n",
    "$S$ independent samples from the marginal posterior distribution of $\\tilde{Y}$ which \n",
    "is the posterior predictive distribution.\n",
    "\n",
    "# 4.4 Posterior predictive model checking\n",
    "\n",
    "We can plot the posterior predictive distribution (made from sampling) and the \n",
    "empirical distribution of our data to see whether they match. You would imagine \n",
    "that the distributions should be similar or else it seems that the predictive\n",
    "distribution would not be good for predicting future values from the empirical\n",
    "distribution.\n",
    "\n",
    "There are a couple resons the empirical (observed) distribution and predictive distribution \n",
    "may not look similar:\n",
    "\n",
    "* the empirical distribution doesn't match the exact distribution of the population from which the data\n",
    "were sampled. This could be due to sampling variability, especially when the number of samples is small\n",
    "* our model is not the correct model for this data\n",
    "\n",
    "We can evaluate these two possibilities by performing repeated simulations of the observed\n",
    "dataset and checking to see how likely the observed data are given repeated samplings of the \n",
    "posterior. Note that you could check \"how likely the observed data are\" in different ways. \n",
    "For instance, you could compare the empirical mean versus the means from the samplings. \n",
    "In the book, they compare the ratio of 1s versus 2s for a Poisson model. The ratio for the\n",
    "observed data looks much different than the ratios from the samplings which suggests the model\n",
    "is not accurately capturing the shape of the distribution. However, just because a model doesn't\n",
    "accurately capture one part of the data doesn't mean it's useless. The model may still recapitulate\n",
    "the mean or variance. For instance, the Poisson model may still provide consistente estimates of\n",
    "the mean and variance for a distribution where the mean and variance are roughly equal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
