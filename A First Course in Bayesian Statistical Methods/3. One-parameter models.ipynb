{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\pr}{\\textrm{Pr}}$\n",
    "$\\newcommand{\\l}{\\left}$\n",
    "$\\newcommand{\\r}{\\right}$\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\var}{\\textrm{Var}}$\n",
    "$\\newcommand{\\mc}{\\mathcal}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\lb}{\\left\\{}$\n",
    "$\\newcommand{\\rb}{\\right\\}}$\n",
    "$\\newcommand{\\iid}{\\textrm{i.i.d. }}$\n",
    "$\\newcommand{\\ev}{\\textrm{E}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 The binomial model\n",
    "\n",
    "For exchangeable binary data, our joint beliefs about the i.i.d. responses $Y_1, \\cdots, Y_n$\n",
    "are well approximated by\n",
    "\n",
    "* our beliefs about $\\theta = \\sum_{i=1}^N Y_i / N$\n",
    "* the model that, conditional on $\\theta$, the $Y_i$'s are i.i.d. binary random variables \n",
    "with expectation $\\theta$\n",
    "\n",
    "If $Y_1, \\cdots, Y_n \\given \\theta$ are i.i.d. binary$\\lp\\theta\\rp$,\n",
    "\n",
    "$$p\\lp\\theta \\given y_1, \\cdots, y_n \\rp = \\theta^{\\sum y_i} \\lp 1-\\theta \\rp^{n-\\sum y_i}\n",
    "\\times p\\lp \\theta \\rp / p\\lp y_1, \\cdots, y_n \\rp$$\n",
    "\n",
    "Given observations $y_1, \\cdots, y_n$, we can use Bayes' rule to calculate \n",
    "$p\\lp \\theta | \\given y_1, \\cdots, y_n \\rp$. If we have a uniform prior, the denominator $p\\lp y_1, \\cdots, y_n \\rp$\n",
    "is known from calculus to be the beta distribution. An uncertain quantity $\\theta$, known \n",
    "to be between 0 and 1, has a $\\textrm{beta}\\lp a,b\\rp$ distribution if\n",
    "$$p\\lp\\theta\\rp = \\textrm{dbeta}\\lp\\theta, a, b\\rp = \n",
    "\\frac{\\Gamma\\lp a+b \\rp}{\\Gamma\\lp a \\rp \\Gamma\\lp b \\rp}\\theta^{a-1} \\lp 1-\\theta \\rp^{b-1}$$\n",
    "\n",
    "for $0 \\leq \\theta \\leq 1$. In the case of our binary data, $a$ is the number of success (e.g. $Y_i = 1$) and \n",
    "$b$ is the number of failures (e.g. $Y_i = 0$). For a random variable with a beta distribution,\n",
    "\n",
    "* $\\textrm{mode}\\l[\\theta\\r] = \\frac{ a-1}{\\lp a-1 \\rp + \\lp b-1 \\rp}$ if $a>1$, $b>1$\n",
    "* $\\ev\\l[\\theta\\r] = \\frac{a}{a + b}$\n",
    "* $\\var\\l[\\theta\\r] = \\frac{ab}{\\lp a+b+1 \\rp\\lp a+b\\rp^2} \n",
    "= \\frac{\\ev\\l[\\theta\\r] \\ev\\l[1 - \\theta\\r]}{a + b + 1}$\n",
    "\n",
    "## 3.1.1 Inference for exchangeable binary data\n",
    "\n",
    "If we compare the relative probabilities of any two $\\theta$ values $\\theta_a$ and $\\theta_b$ \n",
    "by calculating $\\frac{p\\lp\\theta_a \\given y_1, \\cdots, y_n\\rp}{p\\lp\\theta_b \\given y_1, \\cdots, y_n\\rp}$,\n",
    "we will see that the resulting expression is only a function of $\\sum_{i=1}^n y_i$. This means that\n",
    "$\\sum_{i=1}^n Y_i$ contains all the information about $\\theta$ available from the data, so we say\n",
    "that $\\sum_{i=1}^n Y_i$ is a **sufficient statistic** for $\\theta$ and $p\\lp y_1, \\cdots, y_n \\given \\theta\\rp$.\n",
    "Basically, the individual binary responses don't give any more info than knowing in total how many\n",
    "responses were 1. It is sufficient to know $\\sum_{i=1}^n Y_i$ in order to make inferences about $\\theta$. \n",
    "In this case where $Y_1, \\cdots, Y_n \\given \\theta$ are i.i.d. $\\textrm{binary}\\lp\\theta\\rp$ random variables,\n",
    "the sufficient statistic $\\sum_{i=1}^n Y_i$ has a binomial distribution with parameters $\\lp n, \\theta\\rp$.\n",
    "\n",
    "A random variable $Y\\in \\lb 0,1,\\cdots,n\\rb$ has a $\\textrm{binomial}\\lp n, \\theta\\rp$ distribution if\n",
    "$$\\pr\\lp Y=y\\given\\theta\\rp = \\textrm{dbinom}\\lp y,n,\\theta\\rp = {{n}\\choose{y}} \\theta^y\\lp 1 - \\theta\\rp^{n-y},\n",
    "y\\in\\lb 0,1,\\cdots,n\\rb$$\n",
    "* $\\ev\\l[Y \\given\\theta\\r] = n\\theta$\n",
    "* $\\var\\l[Y\\given\\theta\\r] = n\\theta\\lp 1 - \\theta\\rp$\n",
    "\n",
    "### Posterior inference under a uniform prior distribution\n",
    "\n",
    "Having observed $Y=y$, our task is to obtain the posterior distribution of $\\theta$:\n",
    "$$p\\lp \\theta \\given y\\rp = \\frac{p\\lp y \\given \\theta\\rp p\\lp \\theta \\rp}{p\\lp y \\rp}$$\n",
    "Using the binomial pdf and the fact that the denominator is the beta distribution, we \n",
    "find that \n",
    "$$p\\lp\\theta\\given y\\rp = \\textrm{beta}\\lp y + 1, n-y+1 \\rp$$\n",
    "\n",
    "### Posterior distributions under beta prior distributions\n",
    "\n",
    "The uniform prior distribution has $p\\lp\\theta\\rp = 1$ for all $\\theta \\in \\l[0,1\\r]$. \n",
    "This is equivalent to a beta distribution with $a=1,b=1$. Note that $\\Gamma\\lp 1 \\rp = 1$\n",
    "by convention. We saw above that if $\\theta \\sim \\textrm{beta}\\lp 1,1\\rp \\textrm{(uniform)}$\n",
    "and $Y \\sim \\textrm{binomial}\\lp n, \\theta \\rp$, then \n",
    "$\\lb \\theta \\given Y=y \\rp \\sim \\textrm{beta}\\lp 1+y, 1+n-y\\rp$. If we use a beta prior \n",
    "$\\theta \\sim \\textrm{beta}\\lp a,b \\rp$, then\n",
    "$p\\lp\\theta \\given y\\rp = \\textrm{dbeta}\\lp\\theta, a + y, b + n - y\\rp$. So the ones are \n",
    "replaced with $a$ and $b$. We can see this from\n",
    "\\begin{align}\n",
    "p\\lp \\theta \\given y\\rp &= \\frac{p\\lp \\theta \\rp p\\lp y \\given \\theta\\rp}{p\\lp y\\rp} \\\\\n",
    "&= \\frac{1}{p\\lp y \\rp} \\times \\frac{\\Gamma\\lp a+b \\rp}{\\Gamma\\lp a \\rp \\Gamma\\lp b \\rp}\\theta^{a-1} \\lp 1-\\theta \\rp^{b-1} \\times {{n}\\choose{y}} \\theta^y\\lp 1 - \\theta\\rp^{n-y}\n",
    "\\end{align}\n",
    "\n",
    "The first part of this expression $\\frac{1}{p\\lp y \\rp}$ is the normalizing factor. The second part \n",
    "comes from the fact that the prior $p\\lp \\theta \\rp$ is a beta distribution. The third part\n",
    "is the binomial pdf. We can then write\n",
    "\\begin{align}\n",
    "p\\lp \\theta \\given y\\rp &= c\\lp n, y, a, b\\rp \\times\\theta^{a+y-1}\\lp 1 - \\theta\\rp^{b+n-y-1} \\\\\n",
    "&= \\textrm{dbeta}\\lp \\theta, a + y, b + n -y\\rp\n",
    "\\end{align}\n",
    "\n",
    "The first line says that $p\\lp \\theta \\given y\\rp$ is proportional to \n",
    "$\\theta^{a+y-1}\\lp 1 - \\theta\\rp^{b+n-y-1}$ which has the shape of a beta distribution. \n",
    "Since $p\\lp \\theta \\given y\\rp$ and the beta density must both integrate to 1, this means\n",
    "that $p\\lp \\theta \\given y\\rp$ and the beta density are in fact the same. This **trick** \n",
    "will be used throughout the book:\n",
    "\n",
    "* We will recognize the posterior distribution is proportional to a known probability density,\n",
    "and therefore must equal that density.\n",
    "\n",
    "### Conjugacy\n",
    "\n",
    "We have shown that a beta prior distribution and a binomial sampling model lead to a beta\n",
    "posterior distribution. We say that the class of beta priors is **conjugate** for the \n",
    "binomial sampling model.\n",
    "\n",
    "**Definition 4 (Conjugate)**: A class $\\mathcal{P}$ of prior distributions for $\\theta$\n",
    "is called conjugate for a sampling model $p\\lp y\\given\\theta\\rp$ if \n",
    "$$p\\lp \\theta \\rp \\in \\mathcal{P} \\rightarrow p\\lp\\theta\\given y\\rp \\in \\mathcal{P}$$\n",
    "In other words, the distribution is conjugate when the prior and posterior are in the same family.\n",
    "\n",
    "While conjugate priors are easy to calculate, they may not always be the best model.\n",
    "Mixtures of conjugate priors are very flexible and also computationally tractable.\n",
    "\n",
    "### Combining information\n",
    "\n",
    "If $\\theta | Y=y \\sim \\textrm{beta}\\lp a+y, b+n-y\\rp$, then $E\\l[ \\theta \\given y\\r] = \\frac{a+y}{a+b+n}$.\n",
    "This **posterior expectation** is a combination of prior and data information:\n",
    "\\begin{align}\n",
    "E\\l[ \\theta \\given y\\r] &= \\frac{a+y}{a+b+n} \\\\\n",
    "&= \\frac{a+b}{a+b+n}\\frac{a}{a+b} + \\frac{n}{a+b+n}\\frac{y}{n} \\\\\n",
    "&=\\frac{a+b}{a+b+n} \\times \\textrm{ prior expectation } + \\frac{n}{a+b+n} \\times \\textrm{ data average}\n",
    "\\end{align}\n",
    "For this model, the posterior expectation (or posterior mean) is a weighted average of the prior expectation\n",
    "and the sample average, with weights proportional to $a+b$ and $n$ respectively. This leads to an\n",
    "interpretation of $a$ and $b$ as \"prior data\":\n",
    "\n",
    "* $a \\approx$ \"prior number of 1s\"\n",
    "* $b \\approx$ \"prior number of 0s\"\n",
    "* $a + b \\approx$ \"prior sample size\"\n",
    "\n",
    "If our sample size $n$ is much larger than $a+b$, then we would probably want a majority of our\n",
    "information about $\\theta$ to come from the data as opposed to the prior. We can see that is the\n",
    "case if we look back at\n",
    "$$ E\\l[ \\theta \\given y\\r] \n",
    "=\\frac{a+b}{a+b+n} \\times \\textrm{ prior expectation } + \\frac{n}{a+b+n} \\times \\textrm{ data average}$$\n",
    "When $n >> a+b$, $E\\l[ \\theta \\given y\\r]  \\approx \\frac{y}{n}$, the data average.\n",
    "\n",
    "We can also reparameterize $a$ and $b$ in terms of a \"prior sample size\" $n' = a + b$ and \"prior frequency\"\n",
    "$\\theta' = a / n$. This may be easier to use when we are thinking of defining our prior because we can\n",
    "define what we think the frequency might be ($\\theta'$) and how many samples worth of data we are sure\n",
    "($n'$). In particular, we can choose $n'$ based on how many samples we know we will get so we can sort\n",
    "of specify how much weight we are putting on our prior.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "We can create a predictive distribution for data we have yet to observe.\n",
    "Let's consider binary data with $y_1, \\cdots, y_n$ as the observed outcomes\n",
    "of $n$ binary random variables. Let $\\tilde{Y} \\in \\lb0, 1\\rb$ be an additional\n",
    "outcome yet to be observed. The **predictive distributon** of $\\tilde{Y}$ is\n",
    "the conditional distribution of $\\tilde{Y}$ given $\\lb Y_1=y_1, \\cdots, Y_n=y_n\\rb$.\n",
    "For conditionally i.i.d. binary variables this distribution can be derived from\n",
    "the distribution of $\\tilde{Y}$ given $\\theta$ and the posterior distribution of \n",
    "$\\theta$: \n",
    "\\begin{align}\n",
    "\\pr\\lp\\tilde{Y} = 1 \\given y_1, \\cdots, y_n\\rp &= \\ev\\l[\\theta \\given y_1, \\cdots, y_n\\r] \\\\\n",
    "&= \\frac{a + \\sum_{i=1}^n y_i}{a+b+n} \\\\\n",
    "\\pr\\lp\\tilde{Y} = 0 \\given y_1, \\cdots, y_n\\rp &= \\frac{a + \\sum_{i=1}^n \\lp 1-y_i \\rp}{a+b+n}\n",
    "\\end{align}\n",
    "\n",
    "You can see that the predictive distribution does not rely on unknown quantities but does\n",
    "rely on the observed data which makes sense.\n",
    "\n",
    "## 3.1.2 Confidence regions\n",
    "\n",
    "**Definition 5 (Bayesian coverage)**: An interval $\\l[l\\lp y\\rp, u\\lp y\\rp\\r]$, based on the\n",
    "observed data $Y=y$, has 95% Bayesian coverage for $\\theta$ if \n",
    "$$\\pr \\lp l\\lp y\\rp \\leq \\theta \\leq u\\lp y\\rp \\given Y=y\\rp =0.95$$\n",
    "\n",
    "**Definition 6 (frequentist coverage)**: A random interval $\\l[l\\lp Y\\rp, u\\lp Y\\rp\\r]$\n",
    "has 95% frequentist coverage for $\\theta$ if \n",
    "$$\\pr \\lp l\\lp Y\\rp \\leq \\theta \\leq u\\lp Y\\rp \\given \\theta\\rp =0.95$$\n",
    "\n",
    "In a sense, the frequentist and Bayesian notions of coverage describe pre- and post-experimental\n",
    "coverage respectively. \n",
    "\n",
    "My interpretation of this is that when you calculate a frequentist coverage region (same\n",
    "as confidence interval as far as I know), the interpretation is based on *future* experiments. \n",
    "For instance, typically we think of the frequentist 95% confidence interval as having a 95%\n",
    "chance of encompassing the true parameter value. However, the confidence interval actually means\n",
    "that there is a 95% chance that the confidence interval from a *future* experiment will contain\n",
    "the true parameter value. When we perform the experiment, the formula for the confidence interval\n",
    "is specified before the experiment, so the experiment itself is the \"future\" experiment. The\n",
    "calculation of the confidence interval is the realization of the idea that the confidence interval\n",
    "calculated from this \"future\" experiment has a 95% chance of containing the true parameter value.\n",
    "\n",
    "In other words, in the frequentist case, you believe that there is a \"true\" value of $\\theta$, \n",
    "so before you even do your experiment,\n",
    "you expect that the resulting confidence interval will contain the \"true\" value of $\\theta$\n",
    "with 95% probability.\n",
    "\n",
    "The types of intervals constructed in this book are equal to the frequentist confidence intervals\n",
    "asymptotically. Bayesian confidence intervals are also known as **credible regions** or credible intervals.\n",
    "\n",
    "### Quantile-based interval\n",
    "\n",
    "To make a $100 \\times \\lp 1- \\alpha\\rp \\%$ quantile-based confidence interval,\n",
    "find numbers $\\theta_{\\alpha / 2} < \\theta_{1 - \\alpha / 2}$ such that\n",
    "\n",
    "1. $\\pr\\lp \\theta \\leq \\theta_{\\alpha / 2} \\given Y=y\\rp = \\alpha / 2$\n",
    "2. $\\pr\\lp \\theta \\geq \\theta_{1 - \\alpha / 2} \\given Y=y\\rp = \\alpha / 2$\n",
    "\n",
    "The numbers $\\theta_{\\alpha / 2}$, $\\theta_{1 - \\alpha / 2}$ are the $\\alpha / 2$\n",
    "and $1 - \\alpha / 2$ posterior quantiles of $\\theta$, so \n",
    "$$\\pr\\lp \\theta \\in \\l[\\theta_{\\alpha / 2}, \\theta_{1 - \\alpha / 2}\\r] \\given Y=y \\rp = 1 - \\alpha$$\n",
    "\n",
    "### Highest posterior density (HPD) region\n",
    "\n",
    "**Definition 7 (HPD region)**: A $100 \\times \\lp 1-\\alpha\\rp\\%$ HPD region consists of a \n",
    "subset of the parameter space, $s\\lp y\\rp \\subset \\Theta$ such that \n",
    "\n",
    "1. $\\pr\\lp\\theta \\in s\\lp y\\rp \\given Y=y\\rp = 1 - \\alpha$\n",
    "2. If $\\theta_a \\in s\\lp y \\rp$, and $\\theta_b \\notin s\\lp y \\rp$, then \n",
    "$p\\lp \\theta_a \\given Y=y\\rp > p\\lp\\theta_b \\given Y=y\\rp$\n",
    "\n",
    "The first point says that the subset $s\\lp y\\rp$ contains $1-\\alpha$ of the posterior probability.\n",
    "This is the same as the $1-\\alpha$ confidence interval. The second point says that every point in\n",
    "$s\\lp y \\rp$ has a higher posterior probability than every point outside of $s\\lp y \\rp$. This\n",
    "is not necessarily true for the $1-\\alpha$ confidence interval. If the posterior is bimodal, the HPD\n",
    "may not be a continuous region. You can imagine calculate the HPD by starting a horizontal line\n",
    "at the top of a plot of the posterior distribution and moving it down until there is $1-\\alpha$ of the\n",
    "posterior above the line. This would be the HPD region.\n",
    "\n",
    "# 3.2 The Poisson model\n",
    "\n",
    "When our sample space is $\\mathcal{Y} = \\lb 0, 1, 2, \\cdots \\rb$, perhaps the simplest model\n",
    "is the Poisson model. A random variable $Y$ has a Poisson distribution with mean $\\theta$ if\n",
    "$$p\\lp Y=y \\given \\theta \\rp = \\textrm{dpois}\\lp y,\\theta\\rp = \\theta^ye^{-\\theta}/y!$$\n",
    "for $y \\in \\lb 0, 1, 2, \\cdots \\rb$. A Poisson random variable has\n",
    "\n",
    "* $\\ev \\l[Y\\given\\theta\\r] = \\theta$\n",
    "* $\\textrm{Var}\\l[Y\\given\\theta\\r] = \\theta$\n",
    "\n",
    "The Poisson family of distributions is said to have a mean-variance relationship because\n",
    "if one Poisson has a larger mean than another, it will also have a larger variance.\n",
    "\n",
    "## 3.2.1 Posterior inference\n",
    "\n",
    "If we model $Y_1, \\cdots, Y_n$ as i.i.d. Poisson with mean $\\theta$, then the joint pdf\n",
    "of our sample data is\n",
    "\\begin{align}\n",
    "\\pr\\lp Y_1=y_1, \\cdots, Y_n=y_n \\given\\theta\\rp &= \\prod_{i=1}^n p\\lp y_i\\given\\theta\\rp \\\\\n",
    "&=\\prod_{i=1}^n\\frac{1}{y_i!}\\theta^{y_i}e^{-\\theta} \\\\\n",
    "&=c\\lp y_1, \\cdots, y_n \\rp\\theta^{\\sum y_i}e^{-n\\theta}\n",
    "\\end{align}\n",
    "We can compare two values of $\\theta$ *a posteriori* and see\n",
    "\\begin{align}\n",
    "\\frac{p\\lp \\theta_a \\given y \\rp}{p \\lp \\theta_b \\given y \\rp} &= \n",
    "\\frac{e^{-n\\theta_a} \\theta_a^{\\sum y_i} p\\lp\\theta_a\\rp}{e^{-n\\theta_b} \\theta_b^{\\sum y_i} p\\lp\\theta_b\\rp}\n",
    "\\end{align}\n",
    "As with the binary model, $\\sum_{i=1}^n Y_i$ contains all the information about $\\theta$ that is \n",
    "available in the data, so $\\sum_{i=1}^n Y_i$ is a sufficient statistic. Furthermore,\n",
    "$\\lb\\sum_{i=1}^n Y_i\\rb \\sim \\textrm{Poisson}\\lp n\\theta\\rp$.\n",
    "\n",
    "### Conjugate prior\n",
    "\n",
    "A class of prior densities is conjugate for the sampling model $p\\lp y_1, \\cdots, y_n\\rp$ if the\n",
    "posterior distribution is also in the class. For the Poisson sampling model, our posterior for\n",
    "$\\theta$ has the following form\n",
    "\\begin{align}\n",
    "p\\lp y_1, \\cdots, y_n\\rp &\\propto p\\lp \\theta\\rp \\times p\\lp y_1, \\cdots, y_n \\given \\theta\\rp\n",
    "&\\propto p\\lp \\theta\\rp \\times \\theta^{\\sum y_i} e^{-n\\theta}\n",
    "\\end{align}\n",
    "We need to find a conjugate class of densities. The simplest class of densities of this form\n",
    "is known as the family of gamma distributions.\n",
    "\n",
    "### Gamma distribution\n",
    "\n",
    "An uncertain positive quantity $\\theta$ has a $\\textrm{gamma}\\lp a,b \\rp$ distribution if \n",
    "$$p\\lp \\theta \\rp = \\textrm{dgamma}\\lp\\theta,a,b\\rp = \\frac{b^a}{\\Gamma\\lp a\\rp} \\theta^{a-1}e^{-b\\theta}$$\n",
    "for $\\theta,a,b > 0$. For such a random variable\n",
    "\n",
    "* $\\ev\\l[\\theta\\r] = a/b$\n",
    "* $\\var\\l[\\theta\\r] = a/b^2$\n",
    "* $\\textrm{mode}\\l[\\theta\\r] = \\begin{cases} \\lp a-1\\rp/b & \\mbox{if }a>1 \\\\\n",
    "0 & \\mbox{if }a\\leq 1\\end{cases}$\n",
    "\n",
    "### Posterior distribution of $\\theta$\n",
    "\n",
    "The gamma family is conjugate for the Poisson sampling model:\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\theta \\sim \\textrm{gamma}\\lp a,b \\rp \\\\ \n",
    "Y_1, \\cdots, Y_n \\given \\theta \\sim \\textrm{Poisson}\\lp\\theta\\rp\n",
    "\\end{array} \n",
    "\\Rightarrow\n",
    "\\lb \\theta \\given Y_1, \\cdots, Y_n\\rb \\sim \\textrm{gamma}\\lp a + \\sum_{i=1}^n Y_i, b+n\\rp\n",
    "$$\n",
    "The posterior expectation of the $\\theta$ is a convex combination of the prior expectation\n",
    "and the sample average\n",
    "$$\\ev\\l[\\theta\\given y_1, \\cdots, y_n\\r] = \\frac{b}{b+n}\\frac{a}{b} + \\frac{n}{b+n}\\frac{\\sum y_i}{n}$$\n",
    "\n",
    "* $b$ is interpreted as the number of prior observations\n",
    "* $a$ is interpreted as the sum of counts from $b$ prior observations\n",
    "\n",
    "As for the binary model, the information from the data dominates for large $n$.\n",
    "\n",
    "We can derive the posterior predictive distribution as\n",
    "$$p\\lp\\tilde{y} | y_1, \\cdots, y_n\\rp =\n",
    "\\frac{\\Gamma\\lp a + \\sum y_i + \\tilde{y}\\rp}{\\Gamma\\lp\\tilde{y} + 1\\rp\\Gamma\\lp a + \\sum y_i\\rp}\n",
    "\\lp\\frac{b + n}{b + n + 1}\\rp^{a + \\sum y_i}\\lp\\frac{1}{b+n_1}\\rp^\\tilde{y}$$\n",
    "for $y\\in\\lb 0,1,2,\\cdots\\rb$. This is the negative binomial distribution with parameters\n",
    "$\\lp a+\\sum y_i, b+n\\rp$ for which \n",
    "\n",
    "* $\\ev\\l[\\tilde{Y} \\given y_1, \\cdots, y_n\\r] = \\frac{a + \\sum y_i}{b + n} = \\ev\\l[\\theta \\given y_1, \\cdots, y_n\\r]$\n",
    "* $\\var\\l[\\tilde{Y} \\given y_1, \\cdots, y_n\\r] = \\frac{a + \\sum y_i}{b + n}\\frac{b+n+1}{b+n} =\n",
    "\\var\\l[\\theta \\given y_1, \\cdots, y_n\\r] \\times \\lp b+n+1\\rp = \\ev\\l[\\tilde{Y} \\given y_1, \\cdots, y_n\\r] \\times\n",
    "\\frac{b+n+1}{b+n}$\n",
    "\n",
    "The predictive variance is to some extent a measure of our posterior uncertainty about a new sample\n",
    "$\\tilde{Y}$ from the population. Uncertainty about $\\tilde{Y}$ stems from uncertainty about the population\n",
    "(uncertainty in our estimate of $\\theta$) and variability in sampling. For large $n$, the uncertainty about\n",
    "$\\theta$ is small ($\\lp b+n+1\\rp/\\lp b+n\\rp \\approx 1$) and so the predictive variability \n",
    "becomes $\\theta$, the sampling variability from the Poisson. For small $n$, the uncertainty for $\\tilde{Y}$\n",
    "includes additional uncertainty about $\\theta$.\n",
    "\n",
    "# 3.3 Exponential families and conjugate priors\n",
    "\n",
    "The binomial and Poisson models are both instances of one-parameter **exponential family models**.\n",
    "A one-parameter exponential family model is any model whose densities can be expressed as\n",
    "$$p\\lp y\\given \\phi\\rp = h\\lp y\\rp c\\lp\\phi\\rp e^{\\phi t\\lp y\\rp}$$ where $\\phi$ is the unknown\n",
    "parameter and $t\\lp y\\rp$ is the sufficient statistic.\n",
    "\n",
    "Combining priors of the form \n",
    "$$p\\lp \\phi \\given n_0, t_0\\rp = \\kappa\\lp n_0, t_0\\rp c\\lp \\phi \\rp^{n_0} e^{n_0 t_0 \\phi}$$\n",
    "with information from $Y_1, \\cdots, Y_n \\sim \\iid p\\lp y\\given \\phi\\rp$ yields the following\n",
    "posterior distribution\n",
    "\\begin{align}\n",
    "p\\lp \\phi \\given y_1, \\cdots, y_n\\rp &\\propto p\\lp\\phi\\rp p\\lp y_1, \\cdots, y_n \\given \\phi\\rp \\\\\n",
    "&\\propto c\\lp \\phi\\rp^{n_0 + n} \\exp\\lb\\phi\\times\\l[ n_0t_0 + \\sum_{i=1}^n t\\lp y_i \\rp\\r]\\rb \\\\\n",
    "&\\propto p\\lp \\phi \\given n_0 + n, n_0t_0 + n\\bar{t}\\lp \\mathbf{y} \\rp\\rp\n",
    "\\end{align}\n",
    "where $t\\lp \\mathbf{y} \\rp = \\sum t\\lp y_i \\rp/n$. The similarity between the posterior and prior\n",
    "distributions suggests that $n_0$ can be interpreted as a \"prior sample size\" and $t_0$ as a \n",
    "\"prior guess\" of $t\\lp Y\\rp$. In fact\n",
    "$$\\ev\\l[t\\lp Y\\rp\\r] = \\ev\\l[\\ev\\l[t\\lp Y\\rp \\given \\phi\\r]\\r] =\n",
    "\\ev\\l[-c'\\lp \\phi\\rp/c\\lp\\phi\\rp\\r] = t_0$$\n",
    "so $t_0$ represents the expected value of $t\\lp Y\\rp$. The parameter $n_0$ is a measure of how\n",
    "informative the prior is. If you want to think of the relative strength of the prior versus the\n",
    "data, you can think of it that the prior distribution $p\\lp \\phi \\given n_0,t_0\\rp$\n",
    "contains the same information as $n_0$ independent samples from the population.\n",
    "\n",
    "You can take the binomial and Poisson pdfs and rewrite them in the exponential family form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
