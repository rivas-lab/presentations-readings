{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\pr}{\\textrm{Pr}}$\n",
    "$\\newcommand{\\l}{\\left}$\n",
    "$\\newcommand{\\r}{\\right}$\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\var}{\\textrm{Var}}$\n",
    "$\\newcommand{\\mc}{\\mathcal}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\lb}{\\left\\{}$\n",
    "$\\newcommand{\\rb}{\\right\\}}$\n",
    "$\\newcommand{\\iid}{\\textrm{i.i.d. }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Belief functions and probabilities\n",
    "\n",
    "Probabilities are a way to numerically express rational beliefs.\n",
    "This first section shows that probabilities satisfy some general\n",
    "features that we would expect a measure of \"belief\" to have, so\n",
    "it seems reasonable to use probabilities to represent our belief\n",
    "in something.\n",
    "\n",
    "\n",
    "# 2.2 Events, partitions, and Bayes' rule\n",
    "\n",
    "**Bayes' rule**:\n",
    "\\begin{align}\n",
    "\\pr\\left(H_j \\given E\\right) &= \\frac{\\pr\\l(E \\given H_j\\r)\\pr\\l(H_j\\r)}{\\pr\\l(E\\r)} \\\\\n",
    "&= \\frac{\\pr\\l(E \\given H_k\\r)\\pr\\l(H_j\\r)}{\\sum_{k=1}^K \\pr\\l(E \\given H_k\\r)\\pr\\l(H_k\\r)}\n",
    "\\end{align}\n",
    "\n",
    "**Bayes factors**: $\\l\\{H_1, \\cdots, H_k\\r\\}$ often refer to disjoint hypotheses and $E$ refers\n",
    "to data. To compare hypotheses post-experimentally, we often calculate the ratio:\n",
    "\\begin{align}\n",
    "\\frac{\\pr\\l(H_i \\given E\\r)}{\\pr\\l(H_j \\given E\\r)} &= \n",
    "\\frac{\\pr\\l(E \\given H_i\\r)}{\\pr\\l(E \\given H_j\\r)} \\times \\frac{\\pr\\l(H_i\\r)}{\\pr\\l(H_j\\r)} \\\\\n",
    "&= \\textrm{\"Bayes factor\"}\\times \\textrm{\"prior beliefs\"}\n",
    "\\end{align}\n",
    "\n",
    "This quantity reminds us that Bayes' rule does not determine what our beliefs should be after we \n",
    "see data, it only tells us how they should change. \n",
    "\n",
    "\n",
    "# 2.4 Random variables\n",
    "\n",
    "A random variable is an unknown numerical quantity about which we make probability statements.\n",
    "\n",
    "\n",
    "## 2.4.3 Descriptions of distributions\n",
    "\n",
    "\n",
    "### Mean, mode, median\n",
    "\n",
    "The mean is a good quantity to look at because\n",
    "\n",
    "1. The mean is a scaled value of the total of $\\l\\{Y_1, \\cdots, Y_n\\r\\}$, and the total\n",
    "is often a quantity of interest.\n",
    "2. If you were forced to guess the value of $Y$, guessing the mean would minimize your error\n",
    "if it was measured as $\\l(Y - y_{guess}\\r)^2$.\n",
    "3. In some simple models, the mean contains all of the information about the population that \n",
    "can be obtained from the data.\n",
    "\n",
    "### Variance\n",
    " * The variance is the average squared distance that a sample value $Y$ will be from \n",
    "the population mean $E\\l[Y\\r]$. \n",
    " * The standard deviation is the square root of the variance\n",
    "and is on the same scale as the mean.\n",
    "\n",
    "## 2.5 Joint distributions\n",
    "\n",
    "### Discrete distributions\n",
    "\n",
    "The **joint pdf** or **joint density** of discrete random variables $Y_1$ and $Y_2$ is defined as \n",
    "$$p\\l(y_1, y_2\\r) = \\pr\\l(y_1 \\cap y_2 \\r)$$\n",
    "for $y_1 \\in \\mc{Y}_1$, $y_2 \\in \\mc{Y}_2$.\n",
    "\n",
    "The **marginal density** of $Y_1$ can be computed from the joint density of $Y_1$ and $Y_2$:\n",
    "\\begin{align}\n",
    "p\\l(y_1\\r) &= \\sum_{y_2 \\in \\mc{Y}_2}p\\l(y_1, y_2\\r)\n",
    "\\end{align}\n",
    "\n",
    "The **conditional density** of $Y_2$ given $\\l\\{Y_1 = y_1\\r\\}$ can be computed from the \n",
    "joint density and the marginal density:\n",
    "\\begin{align}\n",
    "p\\l(y_2 \\given y_1\\r) &= \\frac{p\\l(y_1, y_2\\r)}{p\\l(y_1\\r)}\n",
    "\\end{align}\n",
    "\n",
    "### Continuous joint distributions\n",
    "\n",
    "If $Y_1$ and $Y_2$ are continuous, the marginal density of $Y_1$ is given by\n",
    "\\begin{align}\n",
    "p\\l(y_1\\r) = \\int_{-\\infty}^\\infty p\\l(y_1,y_2\\r)dy_2\n",
    "\\end{align}\n",
    "and the conditional density is given by\n",
    "\\begin{align}\n",
    "p\\l(y_2 \\given y_1\\r) = p\\l(y_1, y_2\\r) / p\\l(y_1\\r)\n",
    "\\end{align}\n",
    "\n",
    "### Mixed continuous and discrete variables\n",
    "\n",
    "Let $Y_1$ be discrete and $Y_2$ be continuous. Suppose we know $p\\left(y_1\\right)$ and\n",
    "$p\\left(y_2 \\given y_1\\right)$\n",
    "\n",
    "The joint density of $Y_1$ and $Y_2$ is then \n",
    "$$p\\l(y_1, y_2\\r) = p\\l(y_1\\r) \\times p\\l(y_2 \\given y_1\\r)$$\n",
    "\n",
    "and has the property that\n",
    "$$\\pr\\l(Y_1 \\in A, Y_2 \\in B\\r) = \\int_{y_2 \\in B} \\left\\{\\sum_{y_1 \\in A} p_{Y_1Y_2}\\l(y_1, y_2\\r)\\r\\}dy_2$$\n",
    "\n",
    "In other words, we can summation and integration to calculate the joint density.\n",
    "\n",
    "### Bayes rule and parameter estimation\n",
    "\n",
    "Let $\\theta$ be a continuous parameter we want to estimate and let $Y$ be a discrete \n",
    "data measurement. Having observed $\\l\\{Y = y\\r\\}$, we need to compute our updated beliefs about $\\theta$:\n",
    "$$p\\l(\\theta | y\\r) = \\frac{p\\l(\\theta, y\\r)}{p\\lp y\\rp} = \\frac{p\\lp \\theta \\rp p\\lp y \\given \\theta \\rp}{p\\lp y \\rp}$$\n",
    "\n",
    "**This conditional density is called the posterior density of $\\theta$.** If $\\theta_a$ and $\\theta_b$\n",
    "are two estimates of $\\theta$, the posterior probability (density) of $\\theta_a$ relative to $\\theta_b$,\n",
    "conditional on $Y=y$, is \n",
    "\\begin{align}\n",
    "\\frac{p\\lp \\theta_a \\given y \\rp}{p \\lp \\theta_b \\given y \\rp} &= \n",
    "\\frac{p\\lp \\theta_a \\rp p\\lp y | \\theta_a \\rp}{p\\lp \\theta_b \\rp p\\lp y | \\theta_b \\rp}\n",
    "\\end{align}\n",
    "\n",
    "To evaluate the **relative** posterior odds of $\\theta_a$ and $\\theta_b$, we do not need to evaluate\n",
    "the marginal density $p\\lp y \\rp$. We see that\n",
    "$$p\\lp \\theta | y \\rp \\propto p\\lp \\theta \\rp p\\lp y \\given \\theta \\rp$$\n",
    "\n",
    "The constant of proportionality is $1 / p\\lp y \\rp$ which we *could* calculate using\n",
    "$$p\\lp y \\rp = \\int_\\Theta p\\lp y, \\theta \\rp d\\theta = \\int_\\Theta p\\lp y \\given \\theta \\rp p\\lp \\theta \\rp d\\theta$$\n",
    "\n",
    "Later we will see that the numerator is more important than this denominator.\n",
    "\n",
    "\n",
    "# 2.7 Exchangeability\n",
    "\n",
    "The observations in a data set $Y_1, \\cdots, Y_N$ are exchangeable if the subscript labels convey no information about the outcomes. In other words, you could repeatedly permute the data and calculate the joint density and it wouldn't change.\n",
    "\n",
    "Therom: If $Y_1, \\cdots, Y_N$ are conditionally i.i.d. given $\\theta$ and $\\theta \\sim p\\lp \\theta \\rp$,\n",
    "then $Y_1, \\cdots, Y_N$ are exchangeable.\n",
    "\n",
    "# 2.8 de Finetti's thereom\n",
    "\n",
    "We know that if $Y_1, \\cdots, Y_n \\given \\theta$ are i.i.d. and $\\theta \\sim p\\lp \\theta \\rp$, then\n",
    "$Y_1, \\cdots, Y_N$ are exchangeable. de Finetti's theorem says that this statement is if and only if:\n",
    "\n",
    "$$Y_1, \\cdots, Y_n \\given \\theta \\textrm{ are i.i.d. and }\\theta \\sim p\\lp \\theta \\rp \\iff\n",
    "Y_1, \\cdots, Y_N \\textrm{ are exchangeable for all } n$$\n",
    "\n",
    "The question is when are $Y_1, \\cdots, Y_n$ exchangeable for all $n$? For this to be true,\n",
    "we need both exchangeability and repeatability. Exchangeability is true when the labels\n",
    "have no meaning. Repeatability is true when\n",
    "\n",
    "* $Y_1, \\cdots, Y_n$ are outcomes of a repeatable experiment\n",
    "* $Y_1, \\cdots, Y_n$ are sampled from a finite population *with* replacement\n",
    "* $Y_1, \\cdots, Y_n$ are sampled from an infinite population *without* replacement\n",
    "\n",
    "If $Y_1, \\cdots, Y_n$ are exchangeable and sampled from a finite population *without* replacement\n",
    "of size $N >> n$, then they can be modeled as approximately conditionally i.i.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:test2]",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
