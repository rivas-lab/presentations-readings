{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\pr}{\\textrm{Pr}}$\n",
    "$\\newcommand{\\l}{\\left}$\n",
    "$\\newcommand{\\r}{\\right}$\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "$\\newcommand{\\var}{\\textrm{Var}}$\n",
    "$\\newcommand{\\mc}{\\mathcal}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\lb}{\\left\\{}$\n",
    "$\\newcommand{\\rb}{\\right\\}}$\n",
    "$\\newcommand{\\iid}{\\textrm{i.i.d. }}$\n",
    "$\\newcommand{\\ev}{\\textrm{E}}$\n",
    "$\\newcommand{\\odds}{\\textrm{odds}}$\n",
    "$\\newcommand{\\normal}{\\textrm{normal}}$\n",
    "$\\newcommand{\\gamma}{\\textrm{gamma}}$\n",
    "$\\newcommand{\\mode}{\\textrm{mode}}$\n",
    "$\\newcommand{\\MSE}{\\textrm{MSE}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 The normal model\n",
    "\n",
    "A random variable $Y$ is said to be normally distributed with mean $\\theta$ and \n",
    "variance $\\sigma > 0$ if the density of $Y$ is given by\n",
    "$$p\\lp y \\given \\theta, \\sigma \\rp = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\lp\\frac{y-\\theta}{\\sigma}\\rp^2} $$\n",
    "for $-\\infty < y < \\infty$. Some important things about this distribution:\n",
    "\n",
    "* the distribution is symmetric around $\\theta$, and the mode, median, and mean are all equal to $\\theta$\n",
    "* about 95% of the population lies with 95% of the mean (more precisely, 1.96 stdevs)\n",
    "* if $X \\sim \\normal\\lp \\mu, \\tau^2\\rp$, $Y \\sim \\normal\\lp\\theta,\\sigma^2\\rp$, and $X$ and $Y$ are independent, \n",
    "than $aX + bY \\sim \\normal\\lp a\\mu + b\\theta, a^2\\tau^2 + b^2\\sigma^2\\rp$\n",
    "* the R commands for working with normal distribution take the standard deviation $\\sigma$, not the variance\n",
    "$\\sigma^2$ as input\n",
    "\n",
    "The normal distribution is particularly important because the central limit theorem says that \n",
    "under very general conditions, the sum (or mean) of a set of random variables is approximately\n",
    "normally distributed. This means the normal model will be appropriate for data that results\n",
    "from the additive effects of a large number of factors.\n",
    "\n",
    "# 5.2 Inference for the mean, conditional on the variance\n",
    "\n",
    "Suppose our model is $\\lb Y_1, \\cdots, Y_n \\given \\theta, \\sigma^2\\rb \\sim \\iid \\normal\\lp\\theta, \\sigma^2\\rp$.\n",
    "\n",
    "$\\lb \\sum y_i^2, \\sum y_I \\rb$ make up a two-dimensional (vector) sufficient statistic for the \n",
    "normal model. Knowing these values\n",
    "is the same as knowing the sample mean $\\bar{y} = \\sum y_i /n$ and the sample variance\n",
    "$s^2 = \\sum \\lp y_i - \\bar{y} \\rp^2 / \\lp n-1\\rp$, so $\\lb \\bar{y}, s^2 \\rb$ is also a sufficient statistic.\n",
    "\n",
    "A class of prior distributions is conjugate for a sampling model if the resulting posterior\n",
    "distribution is in the same class. If a (conditional) prior distribution $p \\lp \\theta \\given \\sigma^2 \\rp$\n",
    "is normal and $y_1, \\cdots, y_n \\sim \\iid \\normal \\lp\\theta,\\sigma^2\\rp$, the posterior distribution \n",
    "$p\\lp\\theta\\given y_1, \\cdots, y_n, \\sigma^2 \\rp$ will also be normally distributed. If\n",
    "$\\theta \\sim \\normal\\lp\\mu_0,\\tau^2_0\\rp$, then the posterior variance is\n",
    "$$\\tau_n^2 = \\frac{1}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}$$\n",
    "and the posterior mean is\n",
    "$$\\mu_n = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}$$\n",
    "\n",
    "$\\tau_0^2$ is the prior variance and $\\sigma^2$ is the actual sampling variance of the data (how close the $y_i$'s are to $\\theta$). $\\tau_n^2$ is the posterior variance of our estimate of $\\theta$. See next section for more.\n",
    "\n",
    "## Combining information\n",
    "\n",
    "The (conditional) posterior parameters $\\tau_n^2$ and $\\mu_n$ combine the prior parameters $\\tau_0^2$\n",
    "and $\\mu_0$ with terms from the data.\n",
    "\n",
    "### Posterior variance\n",
    "\n",
    "The formula for $1/\\tau_n^2$ is \n",
    "$$\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}$$\n",
    "and so the prior inverse variance is combined with inverse of the data variance.\n",
    "The inverse variance is often referred to as the **precision**. For the normal model, let\n",
    "\n",
    "* $\\tilde{\\sigma}^2 = 1 / \\sigma^2 = $ sampling precision (how close the $y_i$'s are to $\\theta$)\n",
    "* $\\tilde{\\tau}_0^2 = 1 / \\tau_0^2 = $ prior precision \n",
    "* $\\tilde{\\tau}_n^2 = 1 / \\tau_n^2 = $ posterior precision\n",
    "\n",
    "Precision is a quantity that works on an additive scale:\n",
    "$$\\tilde{\\tau}_n^2 = \\tilde{\\tau}_0^2 + n\\tilde{\\sigma}^2$$\n",
    "In other words, posterior information = prior information + data information.\n",
    "Here, our uncertainty in our estimate of $\\theta$ is the sum of our prior uncertainty\n",
    "plus the sampling variability in the data. Here, we assume we know the sampling variability\n",
    "but we will jointly estimate it in the next section.\n",
    "\n",
    "If this is confusing, remember that we are only doing inference for $\\theta$ here, so we will\n",
    "come up with a posterior distribution for $\\theta$. That posterior distribution is normal\n",
    "with mean $\\mu_n$ and variance $\\tau_n^2$. We need these two parameters to define the posterior\n",
    "normal distribution for $\\theta$. In the next section, when we do joint inference \n",
    "for both $\\theta$ and $\\sigma^2$, we will still get a mean and variance for $\\theta$. We will\n",
    "also get one or more parameters that define the posterior distribution of $\\sigma^2$, depending\n",
    "on what the posterior distibution is.\n",
    "\n",
    "### Posterior mean\n",
    "\n",
    "Notice that \n",
    "$$\\mu_n = \\frac{\\tilde{\\tau}_0^2}{\\tilde{\\tau}_0^2 + n\\tilde{\\sigma}^2}\\mu_0 + \n",
    "\\frac{n\\tilde{\\sigma}^2}{\\tilde{\\tau}_0^2 + n\\tilde{\\sigma}^2}\\bar{y}$$\n",
    "so the posterior mean is a weighted average of the prior mean and the sample mean.\n",
    "The weight on the sample mean is $n / \\sigma^2$ (ignoring the denominator), the sampling\n",
    "precision of the sample mean. The weight on the prior mean is $1/\\tau_0^2$, the prior\n",
    "precision. If the prior mean was based On $\\kappa_0$ prior observations from the same \n",
    "or similar population as $Y_1, \\cdots, Y_n$ then we can set $\\tau_0^2 = \\sigma^2 / \\kappa_0$,\n",
    "the variance of the mean of the proir observations. The formula for the posterior mean\n",
    "then reduces to \n",
    "$$\\mu_n = \\frac{\\kappa_0}{\\kappa_0 + n}\\mu_u + \\frac{n}{\\kappa_0 + n}\\bar{y}$$\n",
    "\n",
    "### Prediction\n",
    "\n",
    "If we want to predict a new sample $\\tilde{Y}$ from the population after having observed\n",
    "$Y_1=y_1, \\cdots, Y_n=y_n$, we can use the fact that \n",
    "$$\\lb \\tilde{Y} \\given \\theta, \\sigma^2 \\rb \\sim \\normal\\lp\\theta, \\sigma^2\\rp \\iff\n",
    "\\tilde{Y} = \\theta + \\tilde{\\epsilon}, \\lb\\tilde{\\epsilon} \\given \\theta, \\sigma^2\\rb \\sim \n",
    "\\normal\\lp 0, \\sigma^2\\rp$$\n",
    "In other words, since $\\tilde{Y}$ is normally distributed, it can be represented as its \n",
    "mean plus some normally distributed noise $\\tilde{\\epsilon}$. We can easily compute the \n",
    "predictive mean and variance and find that\n",
    "$$\\tilde{Y} \\given \\sigma^2, y_1, \\cdots, y_n \\sim \\normal\\lp\\mu_n,\\tau_n^2 + \\sigma^2\\rp$$\n",
    "$\\tau_n^2$ is our uncertainty in our estimate for the mean and $\\sigma^2$ is the sampling \n",
    "variability. As our data increases, $\\tau_n^2$ will decrease but the sampling variability\n",
    "will always remain.\n",
    "\n",
    "# 5.3 Joint inference for the mean and variance\n",
    "\n",
    "We perform joint inference in a similar way as for one parameter. Starting with Bayes' rule,\n",
    "\\begin{align}\n",
    "p\\lp\\theta, \\sigma^2\\given y_1,\\cdots,y_n\\rp &= \\frac{p\\lp y_1,\\cdots,y_n \\given \\theta,\\sigma^2\\rp\n",
    "p\\lp\\theta,\\sigma^2\\rp}{p\\lp y_1,\\cdots,y_n \\rp} \\\\\n",
    "&= \\frac{p\\lp y_1,\\cdots,y_n \\given \\theta,\\sigma^2\\rp p\\lp\\theta \\given \\sigma^2\\rp p\\lp \\sigma^2\\rp}\n",
    "{p\\lp y_1,\\cdots,y_n \\rp}\n",
    "\\end{align}\n",
    "We already have a conjugate prior for the mean given the variance so we just need one for \n",
    "the variance $p\\lp \\sigma^2 \\rp$. Let's consider the \n",
    "particular case where $\\tau_0^2 = \\sigma^2 / \\kappa_0$. In other words, the prior variance is \n",
    "equal to the actual sampling variance scaled by the inverse of $\\kappa_0$, our prior sample size. \n",
    "So far we have\n",
    "$$ p\\lp\\theta \\given \\sigma^2\\rp p\\lp \\sigma^2\\rp = \\textrm{dnorm}\\lp\\theta,\\mu_0,\\tau_0 = \\sigma / \\sqrt{\\kappa_0}\\rp\n",
    "\\times p\\lp \\sigma^2\\rp$$\n",
    "We need a prior that has support on $\\lp0, \\infty\\rp$. One such family is the gamma family, though\n",
    "it turns out that the gamma family is conjugate for $1/\\sigma^2$. When using such a prior \n",
    "distriubtion, we say that $\\sigma^2$ has an **inverse-gamma** distribution:\n",
    "\n",
    "* precision = $1/\\sigma^2 \\sim \\textrm{gamma}\\lp a,b\\rp$\n",
    "* variance = $\\sigma^2 \\sim \\textrm{inverse-gamma}\\lp a,b\\rp$\n",
    "\n",
    "We will reparameterize this as \n",
    "$$1/\\sigma^2 \\sim \\gamma\\lp \\frac{\\nu}{2}, \\frac{\\nu}{2}\\sigma_0^2\\rp$$\n",
    "Under this parameterization,\n",
    "* $\\ev\\l[\\sigma^2\\r] = \\sigma_0^2\\frac{\\nu_0 / 2}{\\nu_0 / 2 - 1}$\n",
    "* $\\mode\\l[\\sigma^2\\r] = \\sigma_0^2\\frac{\\nu_0 / 2}{\\nu_0 / 2 + 1}$, so \n",
    "$\\mode\\l[\\sigma^2\\r] < \\sigma_0^2 < \\ev\\l[\\sigma^2\\r]$\n",
    "* $\\var\\l[\\sigma^2\\r]$ is decreasing in $\\nu_0$\n",
    "\n",
    "## Posterior inference\n",
    "\n",
    "In the above section, we decomposed the prior distribution\n",
    "$$p\\lp\\theta,\\sigma^2\\rp = p\\lp\\theta\\given\\sigma^2\\rp p\\lp\\sigma^2\\rp$$\n",
    "Now we will decompose the posterior in the same way\n",
    "$$p\\lp\\theta,\\sigma^2 \\given y_1,\\cdots,y_n\\rp = p\\lp \\theta \\given \\sigma^2, y_1,\\cdots, y_n\\rp\n",
    "p\\lp \\sigma^2 \\rp$$\n",
    "We've already calculated the conditional distribution of $\\theta$ given $\\sigma^2$ and the data\n",
    "above:\n",
    "$$\\lb \\theta \\given y_1,\\cdots,y_n,\\sigma^2\\rb \\sim \\normal\\lp\\mu_n,\\sigma^2/\\kappa_n\\rp$$\n",
    "where\n",
    "$$\\kappa_n = \\kappa_0 + n$$\n",
    "and \n",
    "$$\\mu_n = \\frac{\\lp\\kappa_0 / \\sigma^2\\rp\\mu_0 + \\lp n/\\sigma^2\\rp\\bar{y}}{\\kappa_0/\\sigma^2 + n/\\sigma^2} = \n",
    "\\frac{\\kappa_0\\mu_0 + n\\bar{y}}{\\kappa_n}$$\n",
    "\n",
    "The posterior distribution of $\\sigma^2$ can be obtained by integrating over the unknown value of\n",
    "$\\theta$ which gives\n",
    "$$\\lb 1/\\sigma^2\\given y_1,\\cdots, y_n\\rb\\sim\\gamma\\lp\\nu_n/2,\\nu_n\\sigma^2_n/2\\rp$$\n",
    "where\n",
    "$$\\nu_n = \\nu_0 + n$$\n",
    "and\n",
    "$$\\sigma^2_n = \\frac{1}{\\nu_n}\\l[\\nu_0 \\sigma^2_0 + \\lp n-1 \\rp s^2 + \\frac{\\kappa_0 n}{\\kappa_n}\n",
    "\\lp \\bar{y} - \\mu_0\\rp^2\\r]$$\n",
    "These formulae suggest an interpretation of $\\nu_0$ as a prior sample size, from which a prior\n",
    "sample variance of $\\sigma^2_0$ has been obtained. $s^2$ is the sample variance and $\\lp n-1\\rp s^2$\n",
    "is the sum of the squared observations from the sample mean (sum of squares). We can think of\n",
    "$\\nu_0\\sigma^2_0$ and $\\nu_n\\sigma^2_n$ as the prior and posterior sum of squares, respectively\n",
    "(I suppose because they have the same \"(sample size) $\\times$ (variance)\" form as $\\lp n-1 \\rp s^2$). Multiplying the\n",
    "expression for $\\sigma_n^2$ by $\\nu_n$ almost gives us \"posterior sum of squares equals\n",
    "prior sum of squares plus data sum of squares.\" However, there is a third term \n",
    "$\\frac{\\kappa_0 n}{\\kappa_n} \\lp \\bar{y} - \\mu_0\\rp^2$. This term says that a large value $\\lp \\bar{y} - \\mu_0\\rp^2$\n",
    "increases the posterior probability of a large $\\sigma^2$. This makes sense for our particular\n",
    "joint prior distribution for $\\theta$ and $\\sigma^2$: if we want to think of $\\mu_0$ as the sample mean\n",
    "of $\\kappa_0$ prior observations with variance $\\sigma^2$, then $\\frac{\\kappa_0 n}{\\kappa_n} \\lp \\bar{y} - \\mu_0\\rp^2$\n",
    "is an estimate of $\\sigma^2$ and so we want to use the information this term provides. We will develop\n",
    "an alternative prior distribution in the following section for situations where $\\mu_0$ is not the\n",
    "mean of prior observations.\n",
    "\n",
    "## Monte Carlo sampling\n",
    "\n",
    "Often we are interested in the population mean $\\theta$ and we \n",
    "just want to calculate quantities like $\\ev\\l[y_1\\cdots,y_n\\r]$, $\\textrm{sd}\\l[y_1,\\cdots,y_n\\r]$,\n",
    "$\\pr\\l[\\theta_1 < \\theta_2 \\given y_{1,1},\\cdots,y_{n_2,2}\\r]$, etc. As discussed in the last chapter,\n",
    "we can obtain these quantities by sampling $\\theta$ from the **marginal posterior distribution** of $\\theta$\n",
    "given the data $p\\lp\\theta\\given y_1,\\cdots, y_n\\rp$.\n",
    "\n",
    "Why is this a marginal distribution? It's marginal because we need to marginalize out $\\sigma^2$ (I think).\n",
    "This interpretation would seem to agree with section 4.4. Note that this is sampling from the posterior\n",
    "predictive distribution.\n",
    "\n",
    "So far, we have the conditional distribution of $\\theta$ given $\\sigma^2$ and the data. We can generate samples of\n",
    "$\\theta$ from the joint distribution of $\\theta$ and $\\sigma^2$ by first sampling $\\sigma^2$ from \n",
    "its inverse gamma and then using the sampled $\\sigma^2$ to sample $\\theta$:\n",
    "\\begin{align}\n",
    "\\sigma^{2\\lp 1\\rp} \\sim \\textrm{inverse gamma}\\lp\\nu_0/2, \\sigma^2_n\\nu_n/2\\rp),&\n",
    "\\theta^{\\lp 1 \\rp} \\sim \\normal\\lp\\mu_n, \\sigma^{2\\lp 1\\rp} / \\kappa_n\\rp, \\\\\n",
    "\\cdots, \\\\\n",
    "\\sigma^{2\\lp S\\rp} \\sim \\textrm{inverse gamma}\\lp\\nu_0/2, \\sigma^2_n\\nu_n/2\\rp),&\n",
    "\\theta^{\\lp S \\rp} \\sim \\normal\\lp\\mu_n, \\sigma^{2\\lp S\\rp} / \\kappa_n\\rp, \\\\\n",
    "\\end{align}\n",
    "\n",
    "A sequence of pairs $\\lb\\lp\\\\sigma^{2\\lp 1\\rp}, \\theta^{\\lp 1\\rp}\\rp, \\cdots, \\lp\\\\sigma^{2\\lp S\\rp}, \\theta^{\\lp S\\rp}\\rp\\rb$ simulated in this are way are independent samples from the joint posterior distribution \n",
    "of $p\\lp \\theta, \\sigma^2 \\given y_1,\\cdots,y_n\\rp$. Additionally, the sequence $\\lb \\theta^{\\lp 1\\rp}, \\cdots, \\theta^{\\lp S\\rp}\\rp$ can be seen as independent samples from the marginal posterior distribution of \n",
    "$p\\lp \\theta \\given y_1,\\cdots,y_n\\rp$ (having marginalized $\\sigma^2$ out I suppose). \n",
    "\n",
    "It turns out that the marginal posterior distribution of \n",
    "$$t\\lp\\theta\\rp = \\frac{\\lp\\theta - \\mu_n\\rp}{\\sigma_n / \\sqrt{\\kappa_n}}$$\n",
    "is $t$-distributed with $\\nu_0 + n$\n",
    "degrees of freedom. If $\\kappa_0$ and $\\nu_0$ are small, the posterior distribution\n",
    "of $t\\lp\\theta\\rp$ will be very close to the $t_{n-1}$ distribution.\n",
    "\n",
    "### Improper priors\n",
    "\n",
    "You may be tempted to use Bayesian approaches but try not to use prior information in order\n",
    "to not appear biased. We can let $\\kappa_0$ and $\\nu_0$ go to zero to understand what would\n",
    "happen with no prior information. As $\\kappa_0, \\nu_0 \\rightarrow 0$, \n",
    "\\begin{align}\n",
    "\\mu_n &\\rightarrow \\bar{y} \\\\\n",
    "\\sigma^2_n &\\rightarrow \\frac{1}{n}\\sum \\lp y_i-\\bar{y}\\rp^2\n",
    "\\end{align}\n",
    "This leads to the following posterior distributions:\n",
    "\\begin{align}\n",
    "\\lb 1/\\sigma^2 \\given y_1, \\cdots, y_n \\rb &\\sim \\gamma\\lp \\frac{n}{2}, \\frac{n}{2}\\frac{1}{n}\\sum \\lp y_i-\\bar{y}\\rp^2\\rp \\\\\n",
    "\\lb \\theta \\given y_1, \\cdots, y_n \\rb &\\sim \\normal\\lp\\bar{y},\\frac{\\sigma^2}{n}\\rp\n",
    "\\end{align}\n",
    "You can show that \n",
    "$$\\frac{\\theta - \\bar{y}}{s/\\sqrt{y}}\\given y_1, \\cdots, y_n \\sim t_{n-1}$$\n",
    "\n",
    "This can be compared to the sampling distribution of the $t$ statistic, conditional on $\\theta$\n",
    "but not on the data\n",
    "$$\\frac{\\bar{Y} - \\theta}{s/\\sqrt{n}}\\given\\theta\\sim t_{n-1}$$\n",
    "\n",
    "Ths second statement says that the deviation of the estimate $\\bar{Y}$ from the true population \n",
    "mean $\\theta$ (scaled by the denominator) is represented by a $t_{n-1}$ distribution. The first\n",
    "statement says that after you sample your data, your uncertainty is still represented by $t_{n-1}$\n",
    "distribution. \n",
    "\n",
    "Since there are no prior probabilities that will lead to the $t_{n-1}$ posterior for $\\theta$,\n",
    "inference based on this posterior is not formally Bayesian. Somtimes taking limits like this\n",
    "can lead to reasonable answers, however.\n",
    "\n",
    "# 5.4 Bias, variance, and mean squared error\n",
    "\n",
    "A **point estimator** of an unknown parameter $\\theta$ is a function that converts data into a single\n",
    "element of the parameter space $\\Theta$. In the case of a normal sampling model and conjugate prior\n",
    "distribution of the last section, the posterior mean estimator of $\\theta$ is\n",
    "$$\\hat{\\theta}_b \\lp y_1,\\cdots,y_n\\rp = \\ev\\l[\\theta\\given y_1,\\cdots,y_n\\r] = \n",
    "\\frac{n}{\\kappa_0 + n}\\bar{y} + \\frac{\\kappa_0}{\\kappa_0 + n}\\mu_0 = w\\bar{y} + \\lp 1-w\\rp\\mu_0$$\n",
    "\n",
    "The sampling propertires of an estimator such as $\\hat{\\theta}_b$ refer to its behavior under hypothetically\n",
    "repeatable surveys or experiments. Let's compare the sampling properties of $\\hat{\\theta}_b$ to \n",
    "$\\hat{\\theta}_e\\lp y_1,\\cdots,y_n\\rp = \\bar{y}$, the sample mean, when the true value of the population \n",
    "mean is $\\theta_0$:\n",
    "\\begin{align}\n",
    "\\ev\\l[\\theta_e\\given\\theta=\\theta_0\\r] &= \\theta_0, \\\\\n",
    "\\ev\\l[\\theta_b\\given\\theta=\\theta_0\\r] &= w\\theta_0 + \\lp 1-w\\rp\\mu_0\n",
    "\\end{align}\n",
    "We say that $\\hat{\\theta}_e$ is unbiased because its expected value equals the true population mean.\n",
    "We say that $\\hat{\\theta}_b$ is biased since $\\mu_0 \\not= \\theta_0$.\n",
    "\n",
    "Bias refers to how close the center of mass of the sampling distribution is to the true value. Bias doesn't\n",
    "tell us how far away an estimate from the sampling distribution might be from the true value, however. We\n",
    "can look at the mean squared error (MSE) to evaluate how close an estimator $\\hat{\\theta}$ is likely to be \n",
    "to the true value $\\theta$. Letting $m=\\ev\\l[\\hat{\\theta}\\given\\theta_0\\r]$, the MSE is\n",
    "$$\\MSE\\l[\\hat{\\theta}\\given\\theta_0\\r] = \\var\\l[\\hat{\\theta}\\given\\theta_0\\r] + \n",
    "\\textrm{Bias}\\l[\\hat{\\theta}\\given\\theta_0\\r]$$\n",
    "This means that before the data are gathered, the expected distance from the estimator to the true value\n",
    "depends on how close $\\theta_0$ is to the center of the distribution of $\\hat{\\theta}$ (bias) and how\n",
    "spread out the distribution is (the variance). While the bias $\\hat{\\theta}_e$ is zero, it turns out\n",
    "that $\\var\\l[\\hat{\\theta}_b\\r] < \\var\\l[\\hat{\\theta}_e\\r]$ and that the MSE of $\\hat{\\theta}_b$ \n",
    "is less than the MSE of $\\hat{\\theta}_e$ if \n",
    "\\begin{align}\n",
    "\\lp \\mu_0 - \\theta_0\\rp^2 &< \\frac{\\sigma^2}{n}\\frac{1+w}{1-w} \\\\\n",
    "&=\\sigma^2\\lp\\frac{1}{n}+\\frac{2}{\\kappa_0}\\rp\n",
    "\\end{align}\n",
    "If you know even a little bit about the population you are about to sample from, you should be able to find\n",
    "values of $\\theta_0$ and $\\kappa_0$ such that this inequality holds. In this case, you can construct\n",
    "a Bayesian estimator that will have lower average squared distance to the truth than does the sample mean.\n",
    "For example, if you are pretty sure that your best prior guess $\\mu_0$ is within two standard deviations\n",
    "of the true population mean, then if you pick $\\kappa_0 = 1$ you can be pretty sure the Bayesian\n",
    "estimator has a lower MSE.\n",
    "\n",
    "# 5.5 Prior specification based on expectations\n",
    "\n",
    "A $p$ dimensional exponential family model is a model whose densities can be written as\n",
    "$p\\lp y\\given\\phi\\rp = h\\lp y\\rp c\\lp\\phi\\rp\\exp\\lb\\phi^T \\textbf{t}\\lp y\\rp\\rb$, where\n",
    "$\\phi$ is the parameter to be estimated and $\\textbf{t}\\lp y\\rp = \\lb t_1\\lp y\\rp, \\cdots, t_p\\lp y\\rp\\rb$\n",
    "is the sufficient statistic. The normal model is a two-dimensional exponential family\n",
    "model with\n",
    "* $\\textbf{t}\\lp y\\rp = \\lp y,y^2\\rp$\n",
    "* $\\phi = \\lp \\theta/\\sigma^2, -\\lp 2\\sigma^2\\rp^{-1}\\rp$\n",
    "* $c\\lp\\phi\\rp = \\left|\\phi_2\\right|^{1/2}\\exp\\lb\\phi_1^2/\\lp 2\\phi_2\\rp\\rb$\n",
    "\n",
    "A conjugate prior can be written in terms of $\\phi$, giving\n",
    "$p\\lp\\phi\\given n_0, \\textbf{t}_0\\rp \\propto c\\lp\\phi\\rp^{n_0}\\exp\\lp n_0, \\textbf{t}^T_0\\phi\\rp$, where\n",
    "$\\textbf{t}_0 = \\lp t_{01},t_{02}\\rp = \\lp \\ev\\l[Y\\r], \\ev\\l[Y^2\\r]\\rp$, the prior expectations\n",
    "of $Y$ and $Y^2$. We can reparameterize in terms of $\\theta, \\sigma^2$ and obtain an expression for\n",
    "$p\\lp\\theta,\\sigma^2\\given n_0, t_0\\rp$ that is proportional to a $\\normal\\lp t_{01}, \\sigma^2/n_0\\rp$\n",
    "density times an $\\textrm{inverse-gamma}\\lp\\lp n_0 + 3\\rp/2, n_0\\lp t_2 - t_1^2 / 2\\rp\\rp$ density.\n",
    "\n",
    "How do we interpret the prior parameters $t_{01}$ and $t_{02}$? Consider the case where we have a prior\n",
    "expectation $\\mu_0$ for the population mean and a prior expectation $\\sigma^2$ for the population variance.\n",
    "Our joint distribution for $\\lp\\theta,\\sigma^2\\rp$ is then\n",
    "\\begin{align}\n",
    "\\theta \\given \\sigma^2 &\\sim \\normal\\lp\\mu_0, \\sigma^2 / n_0\\rp \\\\\n",
    "\\sigma^2 &\\sim \\textrm{inverse-gamma}\\lp\\lp n_0 + 3\\rp/2, \\lp n_0+ 1\\rp\\sigma^2_0/2\\rp\n",
    "\\end{align}\n",
    "\n",
    "If our prior information is weak, we might set $n_0=1$.\n",
    "\n",
    "# 5.6 The normal model for non-normal data\n",
    "\n",
    "People use the normal model even for non-normal data because the sampling distribution\n",
    "of the sample mean is generally close to normal. In general, using the normal model\n",
    "for non-normal data is reasonable if we are only interested in obtaining a posterior\n",
    "distribution for the population mean. For other population quantities the normal model\n",
    "can provide misleading results.\n",
    "\n",
    "# 5.7 Discussion and further references\n",
    "\n",
    "A characterizing feature of the normal distribution is that the mean and variance\n",
    "are independent. From a subjective probability standpoint, this suggests if your\n",
    "beliefs about the sample mean are independent from those about the sample variance,\n",
    "then a normal model is appropriate. \n",
    "\n",
    "Among all distribution with a given mean $\\theta$ and variance $\\sigma^2$, the normal\n",
    "distribution is the most diffuse in terms of entropy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
